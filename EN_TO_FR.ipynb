{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672be604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bold_phrases_until_separator(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reads a translation review file, removes the \"**Phrases:**\" section (including the bold formatting)\n",
    "    and its content until the next separator line, and writes the modified content to a new file.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input translation review file.\n",
    "        output_file (str): Path to the output file without \"**Phrases:**\" sections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            content = infile.read()\n",
    "\n",
    "        # Use regex to find and remove the \"**Phrases:**\" block until the separator\n",
    "        modified_content = re.sub(r\"\\*\\*Phrases:\\*\\*\\n(?:- .*\\n)*?(?=\\n-{80,})\", \"\", content, flags=re.DOTALL)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(modified_content)\n",
    "\n",
    "        print(f\"Successfully processed '{input_file}'. The output has been written to '{output_file}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"translation_review.txt\"\n",
    "    output_filename = \"translation_review_modified.txt\"\n",
    "    remove_bold_phrases_until_separator(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecec979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ffmpeg found at: C:\\ffmpeg\\bin\\ffmpeg.EXE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import edge_tts\n",
    "import whisper\n",
    "from shutil import which\n",
    "from pydub import AudioSegment\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "from tempfile import NamedTemporaryFile\n",
    "from deep_translator import GoogleTranslator\n",
    "import ollama\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import shutil\n",
    "import gc\n",
    "import openai\n",
    "import time # Import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "ffmpeg_path = which(\"ffmpeg\")\n",
    "if not ffmpeg_path:\n",
    "    raise RuntimeError(\"ffmpeg not found. Please install ffmpeg first.\")\n",
    "print(f\"✅ ffmpeg found at: {ffmpeg_path}\")\n",
    "\n",
    "SEGMENTS_DIR = \"segments_temp\"\n",
    "os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "video_file_path = \"4.2.2_Flux de navigation_Avr_08_Latest.mp4\"\n",
    "VOICE_CHOICES = [\"fr-CA-SylvieNeural\", \"fr-FR-DeniseNeural\", \"fr-CA-CHantalNeural\"]\n",
    "DEFAULT_VOICE = VOICE_CHOICES[0]\n",
    "DEFAULT_RATE = \"-15%\"\n",
    "OUTPUT_VIDEO = video_file_path+\"_translated_video.mp4\"\n",
    "FINAL_AUDIO_FILE = video_file_path+\"_final_voice.mp3\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c06201",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RATE = \"-15%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a5731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_path = \"4.2.4_Configuration de la solution_Avr_10_Latest.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14d9af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg path: C:\\ffmpeg\\bin\\ffmpeg.EXE\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: FFmpeg Configuration (Adjust path if necessary)\n",
    "os.environ[\"PATH\"] = r\"C:\\ffmpeg\\bin\" + \";\" + os.environ[\"PATH\"]\n",
    "ffmpeg_path = which(\"ffmpeg\")\n",
    "print(f\"FFmpeg path: {ffmpeg_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bd1a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nest_asyncio applied.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Apply nest_asyncio for asynchronous operations\n",
    "nest_asyncio.apply()\n",
    "print(\"nest_asyncio applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cf3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Debugging Functions ---\n",
    "def create_translation_log(debug_entries: list) -> str:\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file_path = f\"translation_debug_{timestamp}.md\"\n",
    "        with open(log_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# Translation Debug Log\\n\\n\")\n",
    "            for entry in debug_entries:\n",
    "                f.write(entry + \"\\n---\\n\")\n",
    "        return log_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create debug log: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfdf8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Functions ---\n",
    "def chunk_text(text: str, max_length: int = 1000) -> list:\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > max_length:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def translate_text(text: str, openai_api_key: str = None) -> str: # Added openai_api_key\n",
    "    try:\n",
    "        if openai_api_key:\n",
    "            print(\"Using OpenAI for translation...\")\n",
    "            return translate_with_openai(text, openai_api_key, target_language=\"fr\")\n",
    "        else:\n",
    "            print(\"Using Google Translator for translation...\")\n",
    "            chunks = chunk_text(text, max_length=512)\n",
    "            translated_chunks = []\n",
    "            for chunk in chunks:\n",
    "                clean_chunk = chunk.strip()\n",
    "                if not clean_chunk:\n",
    "                    continue\n",
    "                try:\n",
    "                    translated = GoogleTranslator(source='auto', target='fr').translate(clean_chunk)\n",
    "                    if not translated.strip():\n",
    "                        raise ValueError(\"Empty translation\")\n",
    "                    translated_chunks.append(translated)\n",
    "                except Exception as e:\n",
    "                    print(f\"Translation failed for chunk: {clean_chunk}. Using original text. Error: {e}\")\n",
    "                    translated_chunks.append(clean_chunk)\n",
    "            return \"\\n\".join(translated_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation process failed: {e}\")\n",
    "        return text\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_translation(text: str) -> str:\n",
    "    text = text.strip()\n",
    "\n",
    "    # 1. If « ... » is present, extract only the content inside\n",
    "    match = re.search(r'«\\s*(.*?)\\s*»', text, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # 2. If not, remove known intro phrases (case-insensitive)\n",
    "    intro_phrases = [\n",
    "        r\"^voici\\s+(une\\s+)?traduction(\\s+possible)?\\s*[:\\-–]*\",\n",
    "        r\"^traduction\\s*[:\\-–]*\",\n",
    "        r\"^la\\s+phrase\\s+traduite\\s+est\\s*[:\\-–]*\",\n",
    "        r\"^version\\s+traduite\\s*[:\\-–]*\",\n",
    "        r\"^on\\s+peut\\s+traduire\\s+cela\\s+par\\s*[:\\-–]*\"\n",
    "    ]\n",
    "    for pattern in intro_phrases:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # 3. Remove any remaining quotation marks (French, English, etc.)\n",
    "    text = re.sub(r'^[«“\"\\']+\\s*', '', text)\n",
    "    text = re.sub(r'\\s*[»”\"\\']+$', '', text)\n",
    "\n",
    "    # 4. Remove redundant line breaks\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\").strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2030ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transcript(transcript: str):\n",
    "    sentence_groups = []\n",
    "    current_group = []\n",
    "    sentence_end_pattern = r'[.!?](?:\\s|$)'\n",
    "    base_segments = []\n",
    "    for line in transcript.splitlines():\n",
    "        line = re.sub(r'^Texte\\s*:\\s*', '', line)\n",
    "        match = re.search(r'(\\d+:\\d+)\\s*-\\s*(\\d+:\\d+):\\s*(.+)$', line)\n",
    "        if match:\n",
    "            start = convert_time(match.group(1))\n",
    "            end = convert_time(match.group(2))\n",
    "            text = match.group(3).strip()\n",
    "            base_segments.append((start, end, text))\n",
    "        else:\n",
    "            print(f\"Line skipped due to incorrect format: {line}\")\n",
    "    if not base_segments:\n",
    "        raise ValueError(\"No valid timestamped segments found in the transcript.\")\n",
    "    for seg_start, seg_end, text in base_segments:\n",
    "        current_group.append((seg_start, seg_end, text))\n",
    "        if re.search(sentence_end_pattern, text):\n",
    "            full_text = ' '.join(t for _, _, t in current_group)\n",
    "            group_start = current_group[0][0]\n",
    "            group_end = current_group[-1][1]\n",
    "            sentence_groups.append((group_start, group_end, full_text))\n",
    "            current_group = []\n",
    "    if current_group:\n",
    "        full_text = ' '.join(t for _, _, t in current_group)\n",
    "        group_start = current_group[0][0]\n",
    "        group_end = current_group[-1][1]\n",
    "        sentence_groups.append((group_start, group_end, full_text))\n",
    "    if not sentence_groups:\n",
    "        raise ValueError(\"No valid sentence groups found in the transcript.\")\n",
    "    return sentence_groups\n",
    "\n",
    "def convert_time(time_str: str) -> int:\n",
    "    m, s = map(int, time_str.split(':'))\n",
    "    return m * 60 + s\n",
    "\n",
    "def convert_seconds_to_time(seconds: int) -> str:\n",
    "    m, s = divmod(seconds, 60)\n",
    "    return f\"{m:02}:{s:02}\"\n",
    "\n",
    "async def generate_segment_audio(text: str, output_file: str, voice: str, rate: str):\n",
    "    if not re.match(r\"^[+-]?\\d+(\\.\\d+)?%$\", rate): # corrected regex\n",
    "        rate = \"-10%\"\n",
    "        print(f\"Invalid rate format. Using default: {rate}\")\n",
    "    communicator = edge_tts.Communicate(text, voice, rate=rate)\n",
    "    try: # added try and except\n",
    "        await communicator.save(output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_segment_audio: {e}\") # Log the error\n",
    "        raise\n",
    "\n",
    "def run_generate_audio_for_segment(text: str, output_file: str, voice: str, rate: str):\n",
    "    nest_asyncio.apply()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(generate_segment_audio(text, output_file, voice, rate))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate audio for segment: {e}\")\n",
    "\n",
    "def generate_transcript(video_path: str) -> str:\n",
    "    print(\"Generating transcript using Whisper...\")\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(video_path)\n",
    "    transcript_lines = []\n",
    "    for segment in result[\"segments\"]:\n",
    "        start_min = int(segment[\"start\"] // 60)\n",
    "        start_sec = int(segment[\"start\"] % 60)\n",
    "        end_min = int(segment[\"end\"] // 60)\n",
    "        end_sec = int(segment[\"end\"] % 60)\n",
    "        text = segment[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "        transcript_lines.append(f\"{start_min:01d}:{start_sec:02d} - {end_min:01d}:{end_sec:02d}: {text}\")\n",
    "    return \"\\n\".join(transcript_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576fe059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synchronized_audio(sentence_groups, voice: str, rate: str, progress_callback=None, translation_model=None, openai_api_key=None): # Added translation model and openai key\n",
    "    from pydub import AudioSegment\n",
    "    import shutil\n",
    "    audio_segments = []\n",
    "    debug_entries = []\n",
    "    total_sentences = len(sentence_groups)\n",
    "    if os.path.exists(SEGMENTS_DIR):\n",
    "        shutil.rmtree(SEGMENTS_DIR)\n",
    "    os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "    total_video_duration = sentence_groups[-1][1] * 1000\n",
    "    cumulative_excess = 0\n",
    "    translated_segments = []\n",
    "\n",
    "    for idx, (start, end, sentence) in enumerate(sentence_groups):\n",
    "        segment_file = os.path.join(SEGMENTS_DIR, f\"sentence_{idx}.mp3\")\n",
    "        if not sentence.strip():\n",
    "            raise ValueError(f\"Empty sentence in group {idx+1}\")\n",
    "\n",
    "        translated = sentence # Default\n",
    "        # Translate each segment based on the user's selection:\n",
    "        if translation_model == \"OpenAI (Cloud)\":\n",
    "            if not openai_api_key:\n",
    "                raise ValueError(\"OpenAI API key is required for translation with OpenAI.\")\n",
    "            translated = translate_with_openai(sentence, openai_api_key, target_language=\"fr\")\n",
    "        elif translation_model == \"Ollama (Local)\":\n",
    "            translated = translate_with_ollama(sentence)\n",
    "        else:\n",
    "            translated = sentence\n",
    "\n",
    "        if not translated.strip():\n",
    "            print(f\"Translation failed for sentence {idx+1}. Using original text.\")\n",
    "            translated = sentence\n",
    "        translated_segments.append(translated)\n",
    "\n",
    "        # Adjust the speaking rate if the translated text is significantly longer than the original.\n",
    "        if len(translated) > len(sentence) * 1.2:\n",
    "            adjusted_rate = f\"{int(rate[:-1]) - 5}%\"\n",
    "            print(f\"Adjusting speaking rate to {adjusted_rate} for segment {idx+1}.\")\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, adjusted_rate)\n",
    "        else:\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, rate)\n",
    "\n",
    "        if not os.path.exists(segment_file) or os.path.getsize(segment_file) == 0:\n",
    "            raise FileNotFoundError(f\"Audio generation failed for sentence {idx+1}\")\n",
    "        try:\n",
    "            segment_audio = AudioSegment.from_file(segment_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Pydub failed to load segment audio: {e}. File: {segment_file}\")\n",
    "            raise\n",
    "\n",
    "        target_duration_ms = (end - start) * 1000\n",
    "        current_duration = len(segment_audio)\n",
    "        tolerance_ms = 200\n",
    "        if abs(current_duration - target_duration_ms) > tolerance_ms:\n",
    "            if current_duration < target_duration_ms:\n",
    "                silence_duration = target_duration_ms - current_duration - cumulative_excess\n",
    "                silence_duration = max(0, silence_duration)\n",
    "                silence = AudioSegment.silent(duration=silence_duration)\n",
    "                segment_audio += silence\n",
    "                cumulative_excess = 0\n",
    "            elif current_duration > target_duration_ms:\n",
    "                segment_audio = segment_audio[:target_duration_ms]\n",
    "                cumulative_excess += current_duration - target_duration_ms\n",
    "        audio_segments.append(segment_audio)\n",
    "        debug_entries.append(\n",
    "            f\"Segment {idx+1} (start: {start}s, end: {end}s):\\n\"\n",
    "            f\"**Original:** {sentence}\\n\"\n",
    "            f\"**Translated:** {translated}\\n\"\n",
    "            f\"**Target duration:** {target_duration_ms/1000:.2f}s, \"\n",
    "            f\"**Audio duration:** {current_duration/1000:.2f}s, \"\n",
    "            f\"**Cumulative excess:** {cumulative_excess/1000:.2f}s\"\n",
    "        )\n",
    "        if progress_callback:\n",
    "            progress = (idx + 1) / total_sentences * 80\n",
    "            progress_callback(progress)\n",
    "\n",
    "    final_audio = sum(audio_segments)\n",
    "    final_duration = len(final_audio)\n",
    "    tolerance_ms = 500 if total_video_duration <= 600000 else 100\n",
    "    if final_duration < total_video_duration - tolerance_ms:\n",
    "        silence = AudioSegment.silent(duration=total_video_duration - final_duration)\n",
    "        final_audio += silence\n",
    "    elif final_duration > total_video_duration + tolerance_ms:\n",
    "        excess_duration = final_duration - total_video_duration\n",
    "        print(f\"Final audio exceeds total video duration by {excess_duration / 1000:.2f}s. Redistributing excess duration.\")\n",
    "        adjustment_ratio = excess_duration / len(audio_segments)\n",
    "        adjusted_segments = []\n",
    "        for segment in audio_segments:\n",
    "            adjusted_duration = len(segment) - adjustment_ratio\n",
    "            adjusted_segments.append(segment[:max(0, int(adjusted_duration))])\n",
    "        final_audio = sum(adjusted_segments)\n",
    "    final_audio_duration = len(final_audio)\n",
    "    if abs(final_audio_duration - total_video_duration) > 100:\n",
    "        if final_audio_duration < total_video_duration:\n",
    "            silence = AudioSegment.silent(duration=total_video_duration - final_audio_duration)\n",
    "            final_audio += silence\n",
    "        elif final_audio_duration > total_video_duration:\n",
    "            final_audio = final_audio[:total_video_duration]\n",
    "    final_audio.export(FINAL_AUDIO_FILE, format=\"mp3\")\n",
    "    if not os.path.exists(FINAL_AUDIO_FILE):\n",
    "        raise RuntimeError(\"Final audio file creation failed\")\n",
    "    if abs(len(AudioSegment.from_file(FINAL_AUDIO_FILE)) - total_video_duration) > 100:\n",
    "        raise ValueError(f\"Final audio duration mismatch: {len(final_audio)/1000:.1f}s vs video {total_video_duration/1000:.1f}s\")\n",
    "    debug_log_path = create_translation_log(debug_entries)\n",
    "    if not debug_log_path:\n",
    "        print(\"Debug log file could not be created.\")\n",
    "    print(\"Final synchronized audio generated!\")\n",
    "    return FINAL_AUDIO_FILE, debug_log_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad42c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcript(transcript_text: str, filename: str = \"transcript.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript_text)\n",
    "    print(f\"Transcript saved to {filename}\")\n",
    "\n",
    "def merge_audio_with_video(video_path: str, audio_path: str):\n",
    "    try:\n",
    "        print(\"Merging audio with video...\")\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = AudioFileClip(audio_path)\n",
    "        output_video_path = OUTPUT_VIDEO\n",
    "        video.set_audio(audio).write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "        if not os.path.exists(output_video_path) or os.path.getsize(output_video_path) == 0:\n",
    "            raise RuntimeError(\"Merged video file is missing or invalid.\")\n",
    "        return output_video_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to merge audio with video: {e}\")\n",
    "        raise\n",
    "\n",
    "def translate_with_openai(text: str, api_key: str, target_language: str = \"fr\") -> str:\n",
    "    try:\n",
    "        openai.api_key = api_key\n",
    "        prompt = f\"\"\"You are a professional translator specializing in ERP Cloud Fusion systems.\n",
    "        Translate the following text into {target_language}, ensuring that technical terms \n",
    "        and user interface elements are accurately translated in the context of ERP Cloud Fusion.\n",
    "\n",
    "        Only return the translated sentence without introductory phrases. \n",
    "        Do not add anything beyond the translation itself.\n",
    "\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "def validate_transcript_format(transcript: str):\n",
    "    for line in transcript.splitlines():\n",
    "        line = re.sub(r'^Texte\\s*:\\s*', '', line)\n",
    "        if not re.match(r'^\\d+:\\d+\\s*-\\s*\\d+:\\d+:\\s*.+$', line):\n",
    "            print(f\"Invalid transcript line format: {line}\")\n",
    "\n",
    "#mistral:instruct\n",
    "#aya\n",
    "\n",
    "def translate_with_ollama(text: str, model: str = \"7shi/llama-translate:8b-q4_K_M\", output_file: str = \"ollama_response.txt\") -> str:\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt = (\n",
    "                f\"Translate the following text into French, ensuring technical terms and UI elements \"\n",
    "                f\"are accurately translated in the context of ERP Cloud Fusion.\\n\\n\"\n",
    "                f\"Only return the translated sentence with no extra formatting or commentary.\\n\\n\"\n",
    "                f\"Text: {text}\"\n",
    "            )\n",
    "        )\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Ollama Response:\\n\")\n",
    "            f.write(str(response))\n",
    "        if \"response\" not in response:\n",
    "            print(\"Unexpected Ollama response format.\")\n",
    "            return text\n",
    "        translated_text = response[\"response\"].strip()\n",
    "        if not translated_text:\n",
    "            print(\"Ollama returned an empty or invalid response.\")\n",
    "            return text\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama Translation failed: {e}\")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9397f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Optional\n",
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "import shutil\n",
    "import os\n",
    "import openai\n",
    "import ollama\n",
    "\n",
    "# Constants (adjust as needed)\n",
    "SEGMENTS_DIR = \"audio_segments\"\n",
    "FINAL_AUDIO_FILE = \"final_audio.mp3\"\n",
    "OUTPUT_VIDEO = \"output_video.mp4\"\n",
    "DEFAULT_VOICE = \"en-US-JennyNeural\"\n",
    "DEFAULT_RATE = \"-10%\"\n",
    "\n",
    "\n",
    "def parse_manual_transcript_line(line: str) -> Optional[Tuple[float, float, str]]:\n",
    "    \"\"\"\n",
    "    Parses a line from the manual transcript file, extracting start time,\n",
    "    end time, and text. Handles potential errors in the time format.\n",
    "    \"\"\"\n",
    "    time_pattern = r\"(\\d+):(\\d+)(?::(\\d+))?\"  # Allow HH:MM or MM:SS or HH:MM:SS\n",
    "    match = re.match(\n",
    "        rf\"^{time_pattern}\\s*-\\s*{time_pattern}:\\s*(.+)$\", line\n",
    "    )\n",
    "    if not match:\n",
    "        print(f\"Skipping invalid transcript line format: {line}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        start_hours, start_minutes, start_seconds, end_hours, end_minutes, end_seconds, text = match.groups()\n",
    "        start_seconds = int(start_seconds) if start_seconds else 0\n",
    "        end_seconds = int(end_seconds) if end_seconds else 0\n",
    "        start_time = (int(start_hours) * 3600 + int(start_minutes) * 60 + start_seconds)\n",
    "        end_time = (int(end_hours) * 3600 + int(end_minutes) * 60 + end_seconds)\n",
    "        return start_time, end_time, text.strip()\n",
    "    except ValueError:\n",
    "        print(f\"Error parsing time values in line: {line}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error parsing transcript line: {e}, line: {line}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "async def generate_segment_audio(text: str, output_file: str, voice: str, rate: str):\n",
    "    \"\"\"Generates audio for a text segment using edge-tts.\"\"\"\n",
    "    if not re.match(r\"^[+-]?\\d+(\\.\\d+)?%$\", rate):\n",
    "        rate = \"-10%\"\n",
    "        print(f\"Invalid rate format. Using default: {rate}\")\n",
    "    communicator = edge_tts.Communicate(text, voice, rate=rate)\n",
    "    try:\n",
    "        await communicator.save(output_file)\n",
    "        if os.path.getsize(output_file) == 0:  # Check for empty file\n",
    "            raise Exception(\"Empty audio file generated\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating audio for segment: {e} (Text: '{text}')\")\n",
    "        print(f\"  Exception type: {type(e)}\")\n",
    "        print(f\"  Exception args: {e.args}\")\n",
    "        raise  # Re-raise the exception to stop the main process\n",
    "\n",
    "\n",
    "def get_ollama_response(text: str) -> str:\n",
    "    \"\"\"Extracts the translated text from an Ollama response.\"\"\"\n",
    "    try:\n",
    "        response_dict = json.loads(text)\n",
    "        if \"response\" in response_dict:\n",
    "            return response_dict[\"response\"].strip()\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    match = re.search(r\"Traduction:\\s*(.+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    match = re.search(r\"response=\\\"(.+?)\\\"\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_translation(text: str) -> str:\n",
    "    \"\"\"Cleans up the translated text by removing extra phrases and whitespace.\"\"\"\n",
    "    text = text.strip()\n",
    "    intro_phrases = [\n",
    "        r\"^voici\\s+(une\\s+)?traduction(\\s+possible)?\\s*[:\\-–]*\",\n",
    "        r\"^traduction\\s*[:\\-–]*\",\n",
    "        r\"^la\\s+phrase\\s+traduite\\s+est\\s*[:\\-–]*\",\n",
    "        r\"^version\\s+traduite\\s*[:\\-–]*\",\n",
    "        r\"^on\\s+peut\\s+traduire\\s+cela\\s+par\\s*[:\\-–]*\",\n",
    "        r\"^translate\\s+the\\s+following\\s+text.*?:\",\n",
    "        r\"^text\\s*[:\\-–]*\",\n",
    "        r\"^ollama response:\\s*\",  # Remove \"Ollama Response:\"\n",
    "    ]\n",
    "    for pattern in intro_phrases:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip()\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def create_synchronized_audio(\n",
    "    sentence_groups: List[Tuple[float, float, str]],\n",
    "    voice: str,\n",
    "    rate: str,\n",
    "    progress_callback: Optional[callable] = None,\n",
    "    translation_model: str = \"Google Translate\",\n",
    "    openai_api_key: Optional[str] = None,\n",
    "    manual_transcript_path: Optional[str] = None,\n",
    "    manual_translations_path: Optional[str] = None,\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Generates synchronized audio for a video, with optional manual transcript\n",
    "    and translations, handling timing from the manual transcript.\n",
    "    \"\"\"\n",
    "\n",
    "    audio_segments = []\n",
    "    debug_entries = []\n",
    "    total_sentences = len(sentence_groups)\n",
    "    if os.path.exists(SEGMENTS_DIR):\n",
    "        shutil.rmtree(SEGMENTS_DIR)\n",
    "    os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "    total_video_duration = sentence_groups[-1][1] * 1000 if sentence_groups else 0\n",
    "    cumulative_excess = 0\n",
    "    translated_segments = []\n",
    "\n",
    "    # Load manual transcript with timing, if provided\n",
    "    manual_transcript_with_timing: Optional[List[Tuple[float, float, str]]] = None\n",
    "    if manual_transcript_path:\n",
    "        try:\n",
    "            manual_transcript_with_timing = []\n",
    "            with open(manual_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    parsed_line = parse_manual_transcript_line(line)\n",
    "                    if parsed_line:\n",
    "                        manual_transcript_with_timing.append(parsed_line)\n",
    "            if not manual_transcript_with_timing:\n",
    "                manual_transcript_with_timing = None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading manual transcript: {e}. Translations will be auto-generated.\")\n",
    "            manual_transcript_with_timing = None\n",
    "\n",
    "    # Load manual translations if provided\n",
    "    manual_translations: Optional[List[str]] = None\n",
    "    if manual_translations_path:\n",
    "        try:\n",
    "            with open(manual_translations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                manual_translations = [line.strip() for line in f.readlines()]\n",
    "            if len(manual_translations) != total_sentences:\n",
    "                print(\n",
    "                    \"WARNING: Number of translations in manual translations file does\"\n",
    "                    \" not match the number of video segments. Translations will be \"\n",
    "                    \"auto-generated.\"\n",
    "                )\n",
    "                manual_translations = None\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error reading manual translations: {e}. Translations will be \"\n",
    "                \"auto-generated.\"\n",
    "            )\n",
    "            manual_translations = None\n",
    "\n",
    "    # Main processing loop\n",
    "    for idx, (video_start, video_end, video_sentence) in enumerate(sentence_groups):\n",
    "        segment_file = os.path.join(SEGMENTS_DIR, f\"sentence_{idx}.mp3\")\n",
    "        original_sentence = video_sentence  # Default to video transcript sentence\n",
    "        translated = \"\"\n",
    "        manual_entry = None\n",
    "\n",
    "        # Find the closest matching manual transcript entry based on time\n",
    "        if manual_transcript_with_timing:\n",
    "            best_match_index = -1\n",
    "            min_time_diff = float('inf')\n",
    "            for i, (manual_start, manual_end, _) in enumerate(manual_transcript_with_timing):\n",
    "                # Calculate the overlap between video segment and manual transcript entry\n",
    "                overlap_start = max(video_start, manual_start)\n",
    "                overlap_end = min(video_end, manual_end)\n",
    "                overlap_duration = max(0, overlap_end - overlap_start)\n",
    "                #if overlap_duration > 0: # Require some overlap\n",
    "                time_diff_start = abs(video_start - manual_start)\n",
    "                time_diff_end = abs(video_end - manual_end)\n",
    "                time_diff = time_diff_start + time_diff_end\n",
    "\n",
    "                if time_diff < min_time_diff:\n",
    "                    min_time_diff = time_diff\n",
    "                    best_match_index = i\n",
    "\n",
    "            if best_match_index != -1:\n",
    "                manual_entry = manual_transcript_with_timing[best_match_index]\n",
    "\n",
    "        if manual_entry:\n",
    "            manual_start, manual_end, manual_text = manual_entry\n",
    "            original_sentence = manual_text\n",
    "            # Use manual translation if available\n",
    "            if manual_translations and best_match_index < len(manual_translations):\n",
    "                translated = manual_translations[best_match_index]\n",
    "                print(f\"Using manual translation for segment {idx + 1}\")\n",
    "            else:\n",
    "                translated = translate_text(original_sentence, translation_model, openai_api_key)\n",
    "                if translation_model == \"Ollama (Local)\":\n",
    "                    translated = get_ollama_response(translated)\n",
    "                translated = clean_translation(translated)\n",
    "            translated_segments.append(translated)\n",
    "            target_duration_ms = (manual_end - manual_start) * 1000\n",
    "            print(f\"  Manual timing: {manual_start}, {manual_end}, duration: {target_duration_ms}\")\n",
    "        else:\n",
    "            translated = translate_text(video_sentence, translation_model, openai_api_key)\n",
    "            if translation_model == \"Ollama (Local)\":\n",
    "                translated = get_ollama_response(translated)\n",
    "            translated = clean_translation(translated)\n",
    "            translated_segments.append(translated)\n",
    "            target_duration_ms = (video_end - video_start) * 1000\n",
    "\n",
    "        # Generate audio\n",
    "        if len(translated) > len(original_sentence) * 1.2:\n",
    "            adjusted_rate = f\"{int(rate[:-1]) - 5}%\"\n",
    "            print(f\"Adjusting speaking rate to {adjusted_rate} for segment {idx+1}.\")\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, adjusted_rate)\n",
    "        else:\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, rate)\n",
    "\n",
    "        if not os.path.exists(segment_file) or os.path.getsize(segment_file) == 0:\n",
    "            raise FileNotFoundError(f\"Audio generation failed for sentence {idx+1}\")\n",
    "        try:\n",
    "            segment_audio = AudioSegment.from_file(segment_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Pydub failed to load segment audio: {e}. File: {segment_file}\")\n",
    "            raise\n",
    "\n",
    "        current_duration = len(segment_audio)\n",
    "        tolerance_ms = 200\n",
    "\n",
    "        if abs(current_duration - target_duration_ms) > tolerance_ms:\n",
    "            if current_duration < target_duration_ms:\n",
    "                silence_duration = target_duration_ms - current_duration - cumulative_excess\n",
    "                silence_duration = max(0, silence_duration)\n",
    "                silence = AudioSegment.silent(duration=silence_duration)\n",
    "                segment_audio += silence\n",
    "                cumulative_excess = 0\n",
    "            elif current_duration > target_duration_ms:\n",
    "                segment_audio = segment_audio[:target_duration_ms]\n",
    "                cumulative_excess += current_duration - target_duration_ms\n",
    "\n",
    "        audio_segments.append(segment_audio)\n",
    "        debug_entries.append(\n",
    "            f\"Segment {idx+1} (video start: {video_start}s, video end: {video_end}s):\\n\"\n",
    "            f\"  Manual start/end/text: {manual_start:.2f} , {manual_end:.2f}, {original_sentence}\\n\"\n",
    "            f\"**Original:** {original_sentence}\\n\"\n",
    "            f\"**Translated:** {translated_segments[idx]}\\n\"\n",
    "            f\"**Target duration:** {target_duration_ms / 1000:.2f}s, \"\n",
    "            f\"**Audio duration:** {current_duration / 1000:.2f}s, \"\n",
    "            f\"**Cumulative excess:** {cumulative_excess / 1000:.2f}\"\n",
    "        )\n",
    "        if progress_callback:\n",
    "            progress = (idx + 1) / total_sentences * 80\n",
    "            progress_callback(progress)\n",
    "\n",
    "    if audio_segments:\n",
    "        final_audio = sum(audio_segments)\n",
    "        final_duration = len(final_audio)\n",
    "        tolerance_ms = 500 if total_video_duration <= 600000 else 100\n",
    "        if final_duration < total_video_duration - tolerance_ms:\n",
    "            silence = AudioSegment.silent(duration=total_video_duration - final_duration)\n",
    "            final_audio += silence\n",
    "        elif final_duration > total_video_duration + tolerance_ms:\n",
    "            excess_duration = final_duration - total_video_duration\n",
    "            print(\n",
    "                \"Final audio exceeds total video duration by \"\n",
    "                f\"{excess_duration / 1000:.2f}s. Redistributing excess duration.\"\n",
    "            )\n",
    "            adjustment_ratio = excess_duration / len(audio_segments)\n",
    "            adjusted_segments = []\n",
    "            for segment in audio_segments:\n",
    "                adjusted_duration = len(segment) - adjustment_ratio\n",
    "                adjusted_segments.append(segment[: max(0, int(adjusted_duration))])\n",
    "            final_audio = sum(adjusted_segments)\n",
    "        final_audio_duration = len(final_audio)\n",
    "        if abs(final_audio_duration - total_video_duration) > 100:\n",
    "            if final_audio_duration < total_video_duration:\n",
    "                silence = AudioSegment.silent(\n",
    "                    duration=total_video_duration - final_audio_duration\n",
    "                )\n",
    "                final_audio += silence\n",
    "            elif final_audio_duration > total_video_duration:\n",
    "                final_audio = final_audio[:total_video_duration]\n",
    "        final_audio.export(FINAL_AUDIO_FILE, format=\"mp3\")\n",
    "        if not os.path.exists(FINAL_AUDIO_FILE):\n",
    "            raise RuntimeError(\"Final audio file creation failed\")\n",
    "        if abs(len(AudioSegment.from_file(FINAL_AUDIO_FILE)) - total_video_duration) > 100:\n",
    "            raise ValueError(\n",
    "                f\"Final audio duration mismatch: {len(final_audio)/1000:.1f}s vs video \"\n",
    "                f\"{total_video_duration/1000:.1f}s\"\n",
    "            )\n",
    "        debug_log_path = create_translation_log(debug_entries)\n",
    "        if not debug_log_path:\n",
    "            print(\"Debug log file could not be created.\")\n",
    "        print(\"Final synchronized audio generated!\")\n",
    "        return FINAL_AUDIO_FILE, debug_log_path\n",
    "    else:\n",
    "        print(\"No audio segments were generated.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "def save_transcript(transcript_text: str, filename: str = \"transcript.txt\"):\n",
    "    \"\"\"Saves the transcript text to a file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript_text)\n",
    "    print(f\"Transcript saved to {filename}\")\n",
    "\n",
    "\n",
    "def merge_audio_with_video(video_path: str, audio_path: str):\n",
    "    \"\"\"Merges the generated audio with the video file.\"\"\"\n",
    "    try:\n",
    "        print(\"Merging audio with video...\")\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = AudioFileClip(audio_path)\n",
    "        output_video_path = OUTPUT_VIDEO\n",
    "        video = video.set_audio(audio)\n",
    "        video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "        video.close()\n",
    "        audio.close()\n",
    "        if not os.path.exists(output_video_path) or os.path.getsize(output_video_path) == 0:\n",
    "            raise RuntimeError(\"Merged video file is missing or invalid.\")\n",
    "        return output_video_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to merge audio with video: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def translate_with_openai(text: str, api_key: str, target_language: str = \"fr\") -> str:\n",
    "    \"\"\"Translates text using OpenAI's GPT-4.\"\"\"\n",
    "    try:\n",
    "        openai.api_key = api_key\n",
    "        prompt = f\"\"\"You are a professional translator specializing in ERP Cloud Fusion systems.\n",
    "        Translate the following text into {target_language}, ensuring that technical terms\n",
    "        and user interface elements are accurately translated in the context of ERP Cloud Fusion.\n",
    "\n",
    "        Only return the translated sentence without introductory phrases.\n",
    "        Do not add anything beyond the translation itself.\n",
    "\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "def validate_transcript_format(transcript: str):\n",
    "    \"\"\"Validates the format of the input transcript.\"\"\"\n",
    "    for line in transcript.splitlines():\n",
    "        line = re.sub(r\"^Texte\\s*:\\s*\", \"\", line)\n",
    "        if not re.match(r\"^\\d+:\\d+\\s*-\\s*\\d+:\\d+:\\s*.+$\", line):\n",
    "            print(f\"Invalid transcript line format: {line}\")\n",
    "\n",
    "\n",
    "\n",
    "def translate_with_ollamaOLD(\n",
    "    text: str, model: str = \"7shi/llama-translate:8b-q4_K_M\", output_file: str = \"ollama_response.txt\"\n",
    ") -> str:\n",
    "    \"\"\"Translates text using a local Ollama model.\"\"\"\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=(\n",
    "                \"Translate the following text into French, ensuring technical terms and UI elements \"\n",
    "                \"are accurately translated in the context of ERP Cloud Fusion.\\n\\n\"\n",
    "                \"Only return the translated sentence with no extra formatting or commentary.\\n\\n\"\n",
    "                \"Text: {text}\"\n",
    "            ),\n",
    "        )\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Ollama Response:\\n\")\n",
    "            f.write(str(response))\n",
    "        if \"response\" not in response:\n",
    "            print(\"Unexpected Ollama response format.\")\n",
    "            return text\n",
    "        translated_text = response[\"response\"].strip()\n",
    "        if not translated_text:\n",
    "            print(\"Ollama returned an empty or invalid response.\")\n",
    "            return text\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def translate_with_ollama(\n",
    "    text: str, model: str = \"7shi/llama-translate:8b-q4_K_M\"\n",
    ") -> str:\n",
    "    \"\"\"Translates text using a local Ollama model and extracts the translated text.\"\"\"\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=(\n",
    "                \"Translate the following text into French, ensuring technical terms and UI elements \"\n",
    "                \"are accurately translated in the context of ERP Cloud Fusion.\\n\\n\"\n",
    "                \"Only return the translated sentence with no extra formatting or commentary.\\n\\n\"\n",
    "                \"Text: {text}\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Extract translated text from Ollama response\n",
    "        try:\n",
    "            response_dict = json.loads(response)\n",
    "            if \"response\" in response_dict:\n",
    "                translated_text = response_dict[\"response\"].strip()\n",
    "                return translated_text\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # If it's not JSON, proceed with regex\n",
    "\n",
    "        patterns = [\n",
    "            r\"Texte\\s*:\\s*(.+)\",  # \"Texte : \"\n",
    "            r\"Traduction\\s*:\\s*(.+)\",  # \"Traduction: \"\n",
    "            r\"response=\\\"(.+?)\\\"\",  # response=\"text\"\n",
    "            r\"^(.+)$\",             # If no specific prefix, take the whole string\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                translated_text = match.group(1).strip()\n",
    "                return translated_text\n",
    "\n",
    "        return response.strip() # Fallback: return the whole response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama Translation failed: {e}\")\n",
    "        return text\n",
    "    \n",
    "def generate_transcript(video_path: str) -> str:\n",
    "    print(\"Extracting audio from video...\")\n",
    "    try:\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        audio_clip = video_clip.audio\n",
    "        audio_temp_file = \"temp_audio.wav\"\n",
    "        audio_clip.write_audiofile(audio_temp_file)\n",
    "        audio_clip.close()\n",
    "        video_clip.close()\n",
    "        print(\"Transcribing audio content...\")\n",
    "        model = whisper.load_model(\"base\")\n",
    "        result = model.transcribe(audio_temp_file)\n",
    "        os.remove(audio_temp_file)\n",
    "        transcript_lines = []\n",
    "        for segment in result[\"segments\"]:\n",
    "            start_min = int(segment[\"start\"] // 60)\n",
    "            start_sec = int(segment[\"start\"] % 60)\n",
    "            end_min = int(segment[\"end\"] // 60)\n",
    "            end_sec = int(segment[\"end\"] % 60)\n",
    "            text = segment[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "            transcript_lines.append(f\"{start_min:01d}:{start_sec:02d} - {end_min:01d}:{end_sec:02d}: {text}\")\n",
    "        return \"\\n\".join(transcript_lines)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcript generation: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f49e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio from video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  33%|███▎      | 7890/23563 [14:28<00:02, 5756.76it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  33%|███▎      | 7890/23563 [14:31<00:02, 5756.76it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribing audio content...\n",
      "Error during transcript generation: [WinError 32] Le processus ne peut pas accéder au fichier car ce fichier est utilisé par un autre processus: 'temp_audio.wav'\n",
      "Original Transcript:\n",
      " \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid timestamped segments found in the transcript.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOriginal Transcript:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, transcript_text)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 3. Parse the transcript\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m sentence_groups = \u001b[43mparse_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscript_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 4. Choose translation model and set API key if needed\u001b[39;00m\n\u001b[32m     12\u001b[39m translation_model = \u001b[33m\"\u001b[39m\u001b[33mOllama (Local)\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Or \"OpenAI (Cloud)\" or \"Google Translate\" or \"Ollama (Local)\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 567\u001b[39m, in \u001b[36mparse_transcript\u001b[39m\u001b[34m(transcript)\u001b[39m\n\u001b[32m    565\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLine skipped due to incorrect format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m base_segments:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo valid timestamped segments found in the transcript.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seg_start, seg_end, text \u001b[38;5;129;01min\u001b[39;00m base_segments:\n\u001b[32m    569\u001b[39m     current_group.append((seg_start, seg_end, text))\n",
      "\u001b[31mValueError\u001b[39m: No valid timestamped segments found in the transcript."
     ]
    }
   ],
   "source": [
    "# 1. Load a video file (replace with your video file)\n",
    "video_file =  \"4.2.4_Configuration de la solution_Avr_10_Latest.mp4\"  # CHANGE THIS\n",
    "\n",
    "# 2. Generate the transcript\n",
    "transcript_text = generate_transcript(video_file)\n",
    "print(\"Original Transcript:\\n\", transcript_text)\n",
    "\n",
    "# 3. Parse the transcript\n",
    "sentence_groups = parse_transcript(transcript_text)\n",
    "\n",
    "# 4. Choose translation model and set API key if needed\n",
    "translation_model = \"Ollama (Local)\"  # Or \"OpenAI (Cloud)\" or \"Google Translate\" or \"Ollama (Local)\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")  # Or your OpenAI API key\n",
    "\n",
    "# 5.  Specify paths for manual transcript and translations (optional)\n",
    "manual_transcript_path = \"4.2.4 _ Configuration de la solutio-ORIGINAL.txt\"  # Optional: Path to manual transcript\n",
    "manual_translations_path = \"4.2.4 _ Configuration de la solutio-TRANSLATED_latest.txt\"  # Optional: Path to manual translations\n",
    "\n",
    "# 6. Generate the translated audio and merge with video\n",
    "audio_file, debug_log_path = create_synchronized_audio(\n",
    "    sentence_groups,\n",
    "    DEFAULT_VOICE,\n",
    "    DEFAULT_RATE,\n",
    "    translation_model=translation_model,\n",
    "    openai_api_key=openai_api_key,\n",
    "    manual_transcript_path=manual_transcript_path,  # Pass the manual transcript path\n",
    "    manual_translations_path=manual_translations_path,  # Pass the manual translations path\n",
    ")\n",
    "print(\"Audio file:\", audio_file)\n",
    "print(\"Debug log:\", debug_log_path)\n",
    "output_video_file = merge_audio_with_video(video_file, audio_file)\n",
    "print(\"Translated video file:\", output_video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9d8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Optional\n",
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "import shutil\n",
    "import os\n",
    "import openai\n",
    "import ollama\n",
    "\n",
    "# Constants (adjust as needed)\n",
    "SEGMENTS_DIR = \"audio_segments\"\n",
    "FINAL_AUDIO_FILE = \"final_audio.mp3\"\n",
    "OUTPUT_VIDEO = \"output_video.mp4\"\n",
    "DEFAULT_VOICE = \"fr-CA-SylvieNeural\"\n",
    "DEFAULT_RATE = \"-10%\"\n",
    "\n",
    "\n",
    "def parse_manual_transcript_line(line: str) -> Optional[Tuple[float, float, str]]:\n",
    "    \"\"\"\n",
    "    Parses a line from the manual transcript file, extracting start time,\n",
    "    end time, and text.  Handles potential errors in the time format.\n",
    "    \"\"\"\n",
    "    time_pattern = r\"(\\d+):(\\d+)(?::(\\d+))?\"  # Allow HH:MM or MM:SS or HH:MM:SS\n",
    "    match = re.match(\n",
    "        rf\"^{time_pattern}\\s*-\\s*{time_pattern}:\\s*(.+)$\", line\n",
    "    )\n",
    "    if not match:\n",
    "        print(f\"Skipping invalid transcript line: {line}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        start_hours, start_minutes, start_seconds, end_hours, end_minutes, end_seconds, text = match.groups()\n",
    "        start_seconds = int(start_seconds) if start_seconds else 0\n",
    "        end_seconds = int(end_seconds) if end_seconds else 0\n",
    "        start_time = (int(start_hours) * 3600 + int(start_minutes) * 60 + start_seconds)\n",
    "        end_time = (int(end_hours) * 3600 + int(end_minutes) * 60 + end_seconds)\n",
    "        return start_time, end_time, text.strip()\n",
    "    except ValueError:\n",
    "        print(f\"Error parsing time values in line: {line}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error parsing transcript line: {e}, line: {line}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "async def generate_segment_audio(text: str, output_file: str, voice: str, rate: str):\n",
    "    \"\"\"Generates audio for a text segment using edge-tts.\"\"\"\n",
    "    if not re.match(r\"^[+-]?\\d+(\\.\\d+)?%$\", rate):\n",
    "        rate = \"-10%\"\n",
    "        print(f\"Invalid rate format. Using default: {rate}\")\n",
    "    communicator = edge_tts.Communicate(text, voice, rate=rate)\n",
    "    try:\n",
    "        await communicator.save(output_file)\n",
    "        if os.path.getsize(output_file) == 0:  # Check for empty file\n",
    "            raise Exception(\"Empty audio file generated\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating audio for segment: {e} (Text: '{text}')\")\n",
    "        print(f\"  Exception type: {type(e)}\")\n",
    "        print(f\"  Exception args: {e.args}\")\n",
    "        raise  # Re-raise the exception to stop the main process\n",
    "\n",
    "\n",
    "def get_ollama_response(text: str) -> str:\n",
    "    \"\"\"Extracts the translated text from an Ollama response.\"\"\"\n",
    "    try:\n",
    "        response_dict = json.loads(text)\n",
    "        if \"response\" in response_dict:\n",
    "            return response_dict[\"response\"].strip()\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    match = re.search(r\"Traduction:\\s*(.+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    match = re.search(r\"response=\\\"(.+?)\\\"\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_translation(text: str) -> str:\n",
    "    \"\"\"Cleans up the translated text by removing extra phrases and whitespace.\"\"\"\n",
    "    text = text.strip()\n",
    "    intro_phrases = [\n",
    "        r\"^voici\\s+(une\\s+)?traduction(\\s+possible)?\\s*[:\\-–]*\",\n",
    "        r\"^traduction\\s*[:\\-–]*\",\n",
    "        r\"^la\\s+phrase\\s+traduite\\s+est\\s*[:\\-–]*\",\n",
    "        r\"^version\\s+traduite\\s*[:\\-–]*\",\n",
    "        r\"^on\\s+peut\\s+traduire\\s+cela\\s+par\\s*[:\\-–]*\",\n",
    "        r\"^translate\\s+the\\s+following\\s+text.*?:\",\n",
    "        r\"^text\\s*[:\\-–]*\",\n",
    "        r\"^ollama response:\\s*\",  # Remove \"Ollama Response:\"\n",
    "    ]\n",
    "    for pattern in intro_phrases:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip()\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def create_synchronized_audio(\n",
    "    sentence_groups: List[Tuple[float, float, str]],\n",
    "    voice: str,\n",
    "    rate: str,\n",
    "    progress_callback: Optional[callable] = None,\n",
    "    translation_model: str = \"Google Translate\",\n",
    "    openai_api_key: Optional[str] = None,\n",
    "    manual_transcript_path: Optional[str] = None,\n",
    "    manual_translations_path: Optional[str] = None,\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Generates synchronized audio for a video, with optional manual transcript\n",
    "    and translations, handling timing from the manual transcript.\n",
    "    \"\"\"\n",
    "\n",
    "    audio_segments = []\n",
    "    debug_entries = []\n",
    "    total_sentences = len(sentence_groups)\n",
    "    if os.path.exists(SEGMENTS_DIR):\n",
    "        shutil.rmtree(SEGMENTS_DIR)\n",
    "    os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "    total_video_duration = sentence_groups[-1][1] * 1000 if sentence_groups else 0\n",
    "    cumulative_excess = 0\n",
    "    translated_segments = []\n",
    "\n",
    "    # Load manual transcript with timing, if provided\n",
    "    manual_transcript_with_timing: Optional[List[Tuple[float, float, str]]] = None\n",
    "    if manual_transcript_path:\n",
    "        try:\n",
    "            manual_transcript_with_timing = []\n",
    "            with open(manual_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    parsed_line = parse_manual_transcript_line(line)\n",
    "                    if parsed_line:\n",
    "                        manual_transcript_with_timing.append(parsed_line)\n",
    "            if not manual_transcript_with_timing:\n",
    "                manual_transcript_with_timing = None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading manual transcript: {e}. Translations will be auto-generated.\")\n",
    "            manual_transcript_with_timing = None\n",
    "\n",
    "    # Load manual translations if provided\n",
    "    manual_translations: Optional[List[str]] = None\n",
    "    if manual_translations_path:\n",
    "        try:\n",
    "            with open(manual_translations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                manual_translations = [line.strip() for line in f.readlines()]\n",
    "            if len(manual_translations) != total_sentences:\n",
    "                print(\n",
    "                    \"WARNING: Number of translations in manual translations file does\"\n",
    "                    \" not match the number of video segments. Translations will be \"\n",
    "                    \"auto-generated.\"\n",
    "                )\n",
    "                manual_translations = None\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error reading manual translations: {e}. Translations will be \"\n",
    "                \"auto-generated.\"\n",
    "            )\n",
    "            manual_translations = None\n",
    "\n",
    "    # Main processing loop\n",
    "    for idx, (video_start, video_end, video_sentence) in enumerate(sentence_groups):\n",
    "        segment_file = os.path.join(SEGMENTS_DIR, f\"sentence_{idx}.mp3\")\n",
    "        original_sentence = video_sentence  # Default to video transcript sentence\n",
    "        translated = \"\"\n",
    "\n",
    "        if manual_transcript_with_timing and idx < len(manual_transcript_with_timing):\n",
    "            manual_start, manual_end, manual_text = manual_transcript_with_timing[idx]\n",
    "            original_sentence = manual_text\n",
    "\n",
    "            # Use manual translation if available\n",
    "            if manual_translations and idx < len(manual_translations):\n",
    "                translated = manual_translations[idx]\n",
    "                print(f\"Using manual translation for segment {idx + 1}\")\n",
    "            else:\n",
    "                translated = translate_text(original_sentence, translation_model, openai_api_key)\n",
    "                if translation_model == \"Ollama (Local)\":\n",
    "                    translated = get_ollama_response(translated)\n",
    "                translated = clean_translation(translated)\n",
    "            translated_segments.append(translated)\n",
    "\n",
    "            # Adjust audio timing to match manual transcript timing\n",
    "            target_duration_ms = (manual_end - manual_start) * 1000\n",
    "            print(f\"  Manual timing: {manual_start}, {manual_end}, duration: {target_duration_ms}\")\n",
    "\n",
    "        else:  # If no manual transcript entry, use original video timing and translation\n",
    "            translated = translate_text(video_sentence, translation_model, openai_api_key)\n",
    "            if translation_model == \"Ollama (Local)\":\n",
    "                translated = get_ollama_response(translated)\n",
    "            translated = clean_translation(translated)\n",
    "            translated_segments.append(translated)\n",
    "            target_duration_ms = (video_end - video_start) * 1000\n",
    "\n",
    "        # Generate audio\n",
    "        if len(translated) > len(original_sentence) * 1.2:\n",
    "            adjusted_rate = f\"{int(rate[:-1]) - 5}%\"\n",
    "            print(f\"Adjusting speaking rate to {adjusted_rate} for segment {idx+1}.\")\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, adjusted_rate)\n",
    "        else:\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, rate)\n",
    "\n",
    "        if not os.path.exists(segment_file) or os.path.getsize(segment_file) == 0:\n",
    "            raise FileNotFoundError(f\"Audio generation failed for sentence {idx+1}\")\n",
    "        try:\n",
    "            segment_audio = AudioSegment.from_file(segment_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Pydub failed to load segment audio: {e}. File: {segment_file}\")\n",
    "            raise\n",
    "\n",
    "        current_duration = len(segment_audio)\n",
    "        tolerance_ms = 200\n",
    "\n",
    "        if abs(current_duration - target_duration_ms) > tolerance_ms:\n",
    "            if current_duration < target_duration_ms:\n",
    "                silence_duration = target_duration_ms - current_duration - cumulative_excess\n",
    "                silence_duration = max(0, silence_duration)\n",
    "                silence = AudioSegment.silent(duration=silence_duration)\n",
    "                segment_audio += silence\n",
    "                cumulative_excess = 0\n",
    "            elif current_duration > target_duration_ms:\n",
    "                segment_audio = segment_audio[:target_duration_ms]\n",
    "                cumulative_excess += current_duration - target_duration_ms\n",
    "\n",
    "        audio_segments.append(segment_audio)\n",
    "        debug_entries.append(\n",
    "            f\"Segment {idx+1} (video start: {video_start}s, video end: {video_end}s):\\n\"\n",
    "            f\"  Manual start/end/text: {manual_start:.2f} , {manual_end:.2f}, {original_sentence}\\n\"\n",
    "            f\"**Original:** {original_sentence}\\n\"\n",
    "            f\"**Translated:** {translated_segments[idx]}\\n\"\n",
    "            f\"**Target duration:** {target_duration_ms / 1000:.2f}s, \"\n",
    "            f\"**Audio duration:** {current_duration / 1000:.2f}s, \"\n",
    "            f\"**Cumulative excess:** {cumulative_excess / 1000:.2f}\"\n",
    "        )\n",
    "        if progress_callback:\n",
    "            progress = (idx + 1) / total_sentences * 80\n",
    "            progress_callback(progress)\n",
    "\n",
    "    if audio_segments:\n",
    "        final_audio = sum(audio_segments)\n",
    "        final_duration = len(final_audio)\n",
    "        tolerance_ms = 500 if total_video_duration <= 600000 else 100\n",
    "        if final_duration < total_video_duration - tolerance_ms:\n",
    "            silence = AudioSegment.silent(duration=total_video_duration - final_duration)\n",
    "            final_audio += silence\n",
    "        elif final_duration > total_video_duration + tolerance_ms:\n",
    "            excess_duration = final_duration - total_video_duration\n",
    "            print(\n",
    "                \"Final audio exceeds total video duration by \"\n",
    "                f\"{excess_duration / 1000:.2f}s. Redistributing excess duration.\"\n",
    "            )\n",
    "            adjustment_ratio = excess_duration / len(audio_segments)\n",
    "            adjusted_segments = []\n",
    "            for segment in audio_segments:\n",
    "                adjusted_duration = len(segment) - adjustment_ratio\n",
    "                adjusted_segments.append(segment[: max(0, int(adjusted_duration))])\n",
    "            final_audio = sum(adjusted_segments)\n",
    "        final_audio_duration = len(final_audio)\n",
    "        if abs(final_audio_duration - total_video_duration) > 100:\n",
    "            if final_audio_duration < total_video_duration:\n",
    "                silence = AudioSegment.silent(\n",
    "                    duration=total_video_duration - final_audio_duration\n",
    "                )\n",
    "                final_audio += silence\n",
    "            elif final_audio_duration > total_video_duration:\n",
    "                final_audio = final_audio[:total_video_duration]\n",
    "        final_audio.export(FINAL_AUDIO_FILE, format=\"mp3\")\n",
    "        if not os.path.exists(FINAL_AUDIO_FILE):\n",
    "            raise RuntimeError(\"Final audio file creation failed\")\n",
    "        if abs(len(AudioSegment.from_file(FINAL_AUDIO_FILE)) - total_video_duration) > 100:\n",
    "            raise ValueError(\n",
    "                f\"Final audio duration mismatch: {len(final_audio)/1000:.1f}s vs video \"\n",
    "                f\"{total_video_duration/1000:.1f}s\"\n",
    "            )\n",
    "        debug_log_path = create_translation_log(debug_entries)\n",
    "        if not debug_log_path:\n",
    "            print(\"Debug log file could not be created.\")\n",
    "        print(\"Final synchronized audio generated!\")\n",
    "        return FINAL_AUDIO_FILE, debug_log_path\n",
    "    else:\n",
    "        print(\"No audio segments were generated.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "def save_transcript(transcript_text: str, filename: str = \"transcript.txt\"):\n",
    "    \"\"\"Saves the transcript text to a file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript_text)\n",
    "    print(f\"Transcript saved to {filename}\")\n",
    "\n",
    "\n",
    "def merge_audio_with_video(video_path: str, audio_path: str):\n",
    "    \"\"\"Merges the generated audio with the video file.\"\"\"\n",
    "    try:\n",
    "        print(\"Merging audio with video...\")\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = AudioFileClip(audio_path)\n",
    "        output_video_path = OUTPUT_VIDEO\n",
    "        video = video.set_audio(audio)\n",
    "        video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "        video.close()\n",
    "        audio.close()\n",
    "        if not os.path.exists(output_video_path) or os.path.getsize(output_video_path) == 0:\n",
    "            raise RuntimeError(\"Merged video file is missing or invalid.\")\n",
    "        return output_video_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to merge audio with video: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def translate_with_openai(text: str, api_key: str, target_language: str = \"fr\") -> str:\n",
    "    \"\"\"Translates text using OpenAI's GPT-4.\"\"\"\n",
    "    try:\n",
    "        openai.api_key = api_key\n",
    "        prompt = f\"\"\"You are a professional translator specializing in ERP Cloud Fusion systems.\n",
    "        Translate the following text into {target_language}, ensuring that technical terms\n",
    "        and user interface elements are accurately translated in the context of ERP Cloud Fusion.\n",
    "\n",
    "        Only return the translated sentence without introductory phrases.\n",
    "        Do not add anything beyond the translation itself.\n",
    "\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "def validate_transcript_format(transcript: str):\n",
    "    \"\"\"Validates the format of the input transcript.\"\"\"\n",
    "    for line in transcript.splitlines():\n",
    "        line = re.sub(r\"^Texte\\s*:\\s*\", \"\", line)\n",
    "        if not re.match(r\"^\\d+:\\d+\\s*-\\s*\\d+:\\d+:\\s*.+$\", line):\n",
    "            print(f\"Invalid transcript line format: {line}\")\n",
    "\n",
    "\n",
    "\n",
    "def translate_with_ollama(\n",
    "    text: str, model: str = \"7shi/llama-translate:8b-q4_K_M\", output_file: str = \"ollama_response.txt\"\n",
    ") -> str:\n",
    "    \"\"\"Translates text using a local Ollama model.\"\"\"\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=(\n",
    "                \"Translate the following text into French, ensuring technical terms and UI elements \"\n",
    "                \"are accurately translated in the context of ERP Cloud Fusion.\\n\\n\"\n",
    "                \"Only return the translated sentence with no extra formatting or commentary.\\n\\n\"\n",
    "                \"Text: {text}\"\n",
    "            ),\n",
    "        )\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Ollama Response:\\n\")\n",
    "            f.write(str(response))\n",
    "        if \"response\" not in response:\n",
    "            print(\"Unexpected Ollama response format.\")\n",
    "            return text\n",
    "        translated_text = response[\"response\"].strip()\n",
    "        if not translated_text:\n",
    "            print(\"Ollama returned an empty or invalid response.\")\n",
    "            return text\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama Translation failed: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e081a",
   "metadata": {},
   "source": [
    "FINAL HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9e526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ffmpeg found at: C:\\ffmpeg\\bin\\ffmpeg.EXE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import edge_tts\n",
    "import whisper\n",
    "from shutil import which\n",
    "from pydub import AudioSegment\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "from tempfile import NamedTemporaryFile\n",
    "from deep_translator import GoogleTranslator\n",
    "import ollama\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import shutil\n",
    "import gc\n",
    "import openai\n",
    "import time # Import time\n",
    "\n",
    "# --- Configuration ---\n",
    "ffmpeg_path = which(\"ffmpeg\")\n",
    "if not ffmpeg_path:\n",
    "    raise RuntimeError(\"ffmpeg not found. Please install ffmpeg first.\")\n",
    "print(f\"✅ ffmpeg found at: {ffmpeg_path}\")\n",
    "\n",
    "SEGMENTS_DIR = \"segments_temp\"\n",
    "os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "\n",
    "VOICE_CHOICES = [\"fr-CA-SylvieNeural\", \"fr-FR-DeniseNeural\", \"fr-CA-CHantalNeural\"]\n",
    "DEFAULT_VOICE = VOICE_CHOICES[0]\n",
    "DEFAULT_RATE = \"-10%\"\n",
    "OUTPUT_VIDEO = \"translated_video.mp4\"\n",
    "FINAL_AUDIO_FILE = \"final_voice.mp3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "async def generate_segment_audio(text: str, output_file: str, voice: str, rate: str):\n",
    "    if not re.match(r\"^[+-]?\\d+(\\.\\d+)?%$\", rate):\n",
    "        rate = \"-10%\"\n",
    "        print(f\"Invalid rate format. Using default: {rate}\")\n",
    "    communicator = edge_tts.Communicate(text, voice, rate=rate)\n",
    "    try:\n",
    "        await communicator.save(output_file)\n",
    "        if os.path.getsize(output_file) == 0:  # Check for empty file\n",
    "            raise Exception(\"Empty audio file generated\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating audio for segment: {e} (Text: '{text}')\")\n",
    "        print(f\"  Exception type: {type(e)}\")  # Print exception type\n",
    "        print(f\"  Exception args: {e.args}\")    # Print exception arguments\n",
    "        raise  # Re-raise the exception to stop the main process\n",
    "\n",
    "def get_ollama_response(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the translated text from an Ollama response.  Handles variations\n",
    "    in the response format.\n",
    "    \"\"\"\n",
    "    # Try to load the text as a JSON object\n",
    "    try:\n",
    "        response_dict = json.loads(text)\n",
    "        if \"response\" in response_dict:\n",
    "            return response_dict[\"response\"].strip()\n",
    "    except json.JSONDecodeError:\n",
    "        pass  # If it's not valid JSON, continue to regex parsing\n",
    "\n",
    "    # Fallback to regex parsing (more robust)\n",
    "    match = re.search(r\"Traduction:\\s*(.+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    match = re.search(r\"response=\\\"(.+?)\\\"\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    return text  # Return original if no pattern found\n",
    "\n",
    "def clean_translation(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    intro_phrases = [\n",
    "        r\"^voici\\s+(une\\s+)?traduction(\\s+possible)?\\s*[:\\-–]*\",\n",
    "        r\"^traduction\\s*[:\\-–]*\",\n",
    "        r\"^la\\s+phrase\\s+traduite\\s+est\\s*[:\\-–]*\",\n",
    "        r\"^version\\s+traduite\\s*[:\\-–]*\",\n",
    "        r\"^on\\s+peut\\s+traduire\\s+cela\\s+par\\s*[:\\-–]*\",\n",
    "        r\"^translate\\s+the\\s+following\\s+text.*?:\",\n",
    "        r\"^text\\s*[:\\-–]*\",\n",
    "        r\"^ollama response:\\s*\",  # Remove \"Ollama Response:\"\n",
    "    ]\n",
    "    for pattern in intro_phrases:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip()\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def create_synchronized_audioOld(sentence_groups, voice, rate, progress_callback=None, translation_model=\"Google Translate\", openai_api_key=None):\n",
    "    from pydub import AudioSegment\n",
    "    import shutil\n",
    "    audio_segments = []\n",
    "    debug_entries = []\n",
    "    total_sentences = len(sentence_groups)\n",
    "    if os.path.exists(SEGMENTS_DIR):\n",
    "        shutil.rmtree(SEGMENTS_DIR)\n",
    "    os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "    total_video_duration = sentence_groups[-1][1] * 1000 if sentence_groups else 0\n",
    "    cumulative_excess = 0\n",
    "    translated_segments = []\n",
    "\n",
    "    for idx, (start, end, sentence) in enumerate(sentence_groups):\n",
    "        segment_file = os.path.join(SEGMENTS_DIR, f\"sentence_{idx}.mp3\")\n",
    "        if not sentence.strip():\n",
    "            raise ValueError(f\"Empty sentence in group {idx+1}\")\n",
    "\n",
    "        translated = translate_text(sentence, translation_model, openai_api_key)\n",
    "        if translation_model == \"Ollama (Local)\":\n",
    "            translated = get_ollama_response(translated)\n",
    "        translated = clean_translation(translated)\n",
    "        translated_segments.append(translated) # Keep track of the cleaned translations\n",
    "\n",
    "        # Adjust the speaking rate if the translated text is significantly longer than the original.\n",
    "        if len(translated) > len(sentence) * 1.2:\n",
    "            adjusted_rate = f\"{int(rate[:-1]) - 5}%\"\n",
    "            print(f\"Adjusting speaking rate to {adjusted_rate} for segment {idx+1}.\")\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, adjusted_rate)\n",
    "        else:\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, rate)\n",
    "\n",
    "        if not os.path.exists(segment_file) or os.path.getsize(segment_file) == 0:\n",
    "            raise FileNotFoundError(f\"Audio generation failed for sentence {idx+1}\")\n",
    "        try:\n",
    "            segment_audio = AudioSegment.from_file(segment_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Pydub failed to load segment audio: {e}. File: {segment_file}\")\n",
    "            raise\n",
    "\n",
    "        target_duration_ms = (end - start) * 1000\n",
    "        current_duration = len(segment_audio)\n",
    "        tolerance_ms = 200\n",
    "        if abs(current_duration - target_duration_ms) > tolerance_ms:\n",
    "            if current_duration < target_duration_ms:\n",
    "                silence_duration = target_duration_ms - current_duration - cumulative_excess\n",
    "                silence_duration = max(0, silence_duration)\n",
    "                silence = AudioSegment.silent(duration=silence_duration)\n",
    "                segment_audio += silence\n",
    "                cumulative_excess = 0\n",
    "            elif current_duration > target_duration_ms:\n",
    "                segment_audio = segment_audio[:target_duration_ms]\n",
    "                cumulative_excess += current_duration - target_duration_ms\n",
    "        audio_segments.append(segment_audio)\n",
    "        debug_entries.append(\n",
    "            f\"Segment {idx+1} (start: {start}s, end: {end}s):\\n\"\n",
    "            f\"**Original:** {sentence}\\n\"\n",
    "            f\"**Translated:** {translated_segments[idx]}\\n\" # Use the cleaned translation\n",
    "            f\"**Target duration:** {target_duration_ms/1000:.2f}s, \"\n",
    "            f\"**Audio duration:** {current_duration/1000:.2f}s, \"\n",
    "            f\"**Cumulative excess:** {cumulative_excess/1000:.2f}\"\n",
    "        )\n",
    "        if progress_callback:\n",
    "            progress = (idx + 1) / total_sentences * 80\n",
    "            progress_callback(progress)\n",
    "\n",
    "    if audio_segments:\n",
    "        final_audio = sum(audio_segments)\n",
    "        final_duration = len(final_audio)\n",
    "        tolerance_ms = 500 if total_video_duration <= 600000 else 100\n",
    "        if final_duration < total_video_duration - tolerance_ms:\n",
    "            silence = AudioSegment.silent(duration=total_video_duration - final_duration)\n",
    "            final_audio += silence\n",
    "        elif final_duration > total_video_duration + tolerance_ms:\n",
    "            excess_duration = final_duration - total_video_duration\n",
    "            print(f\"Final audio exceeds total video duration by {excess_duration / 1000:.2f}s. Redistributing excess duration.\")\n",
    "            adjustment_ratio = excess_duration / len(audio_segments)\n",
    "            adjusted_segments = []\n",
    "            for segment in audio_segments:\n",
    "                adjusted_duration = len(segment) - adjustment_ratio\n",
    "                adjusted_segments.append(segment[:max(0, int(adjusted_duration))])\n",
    "            final_audio = sum(adjusted_segments)\n",
    "        final_audio_duration = len(final_audio)\n",
    "        if abs(final_audio_duration - total_video_duration) > 100:\n",
    "            if final_audio_duration < total_video_duration:\n",
    "                silence = AudioSegment.silent(duration=total_video_duration - final_audio_duration)\n",
    "                final_audio += silence\n",
    "            elif final_audio_duration > total_video_duration:\n",
    "                final_audio = final_audio[:total_video_duration]\n",
    "        final_audio.export(FINAL_AUDIO_FILE, format=\"mp3\")\n",
    "        if not os.path.exists(FINAL_AUDIO_FILE):\n",
    "            raise RuntimeError(\"Final audio file creation failed\")\n",
    "        if abs(len(AudioSegment.from_file(FINAL_AUDIO_FILE)) - total_video_duration) > 100:\n",
    "            raise ValueError(f\"Final audio duration mismatch: {len(final_audio)/1000:.1f}s vs video {total_video_duration/1000:.1f}s\")\n",
    "        debug_log_path = create_translation_log(debug_entries)\n",
    "        if not debug_log_path:\n",
    "            print(\"Debug log file could not be created.\")\n",
    "        print(\"Final synchronized audio generated!\")\n",
    "        return FINAL_AUDIO_FILE, debug_log_path\n",
    "    else:\n",
    "        print(\"No audio segments were generated.\")\n",
    "        return None, None\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "def create_synchronized_audio(\n",
    "    sentence_groups: List[Tuple[float, float, str]],\n",
    "    voice: str,\n",
    "    rate: str,\n",
    "    progress_callback: Optional[callable] = None,\n",
    "    translation_model: str = \"Google Translate\",\n",
    "    openai_api_key: Optional[str] = None,\n",
    "    manual_transcript_path: Optional[str] = None,  # Added for manual transcript\n",
    "    manual_translations_path: Optional[str] = None,  # Added for manual translations\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Generates synchronized audio for a video, with optional manual transcript and translations.\n",
    "\n",
    "    Args:\n",
    "        sentence_groups: A list of tuples, where each tuple contains the start time,\n",
    "                         end time, and sentence text for a segment.\n",
    "        voice: The voice to use for speech synthesis.\n",
    "        rate: The speaking rate.\n",
    "        progress_callback: An optional callback function to track progress.\n",
    "        translation_model: The translation model to use (\"Google Translate\" or \"Ollama (Local)\").\n",
    "        openai_api_key: The API key for OpenAI (if using OpenAI).\n",
    "        manual_transcript_path: Path to a text file containing the manual transcript.\n",
    "        manual_translations_path: Path to a text file containing manual translations\n",
    "                                  (one translation per line, corresponding to sentences).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the path to the final audio file and the path to the debug log.\n",
    "    \"\"\"\n",
    "    from pydub import AudioSegment\n",
    "    import shutil\n",
    "\n",
    "    audio_segments = []\n",
    "    debug_entries = []\n",
    "    total_sentences = len(sentence_groups)\n",
    "    if os.path.exists(SEGMENTS_DIR):\n",
    "        shutil.rmtree(SEGMENTS_DIR)\n",
    "    os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "    total_video_duration = sentence_groups[-1][1] * 1000 if sentence_groups else 0\n",
    "    cumulative_excess = 0\n",
    "    translated_segments = []\n",
    "\n",
    "    # Load manual transcript if provided\n",
    "    manual_transcript: Optional[List[str]] = None\n",
    "    if manual_transcript_path:\n",
    "        try:\n",
    "            with open(manual_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                manual_transcript = [line.strip() for line in f.readlines()]\n",
    "            if len(manual_transcript) != total_sentences:\n",
    "                print(\n",
    "                    \"WARNING: Number of sentences in manual transcript does not match\"\n",
    "                    \" the number of video segments. Translations will be auto-generated.\"\n",
    "                )\n",
    "                manual_transcript = None  # Ignore the manual transcript\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading manual transcript: {e}. Translations will be auto-generated.\")\n",
    "            manual_transcript = None\n",
    "\n",
    "    # Load manual translations if provided\n",
    "    manual_translations: Optional[List[str]] = None\n",
    "    if manual_translations_path:\n",
    "        try:\n",
    "            with open(manual_translations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                manual_translations = [line.strip() for line in f.readlines()]\n",
    "            if len(manual_translations) != total_sentences:\n",
    "                print(\n",
    "                    \"WARNING: Number of translations in manual translations file does\"\n",
    "                    \" not match the number of video segments. Translations will be \"\n",
    "                    \"auto-generated.\"\n",
    "                )\n",
    "                manual_translations = None  # Ignore manual translations\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error reading manual translations: {e}. Translations will be \"\n",
    "                \"auto-generated.\"\n",
    "            )\n",
    "            manual_translations = None\n",
    "\n",
    "    for idx, (start, end, sentence) in enumerate(sentence_groups):\n",
    "        segment_file = os.path.join(SEGMENTS_DIR, f\"sentence_{idx}.mp3\")\n",
    "        if not sentence.strip():\n",
    "            raise ValueError(f\"Empty sentence in group {idx+1}\")\n",
    "\n",
    "        # Use manual translation if provided, otherwise, translate automatically\n",
    "        if manual_translations and idx < len(manual_translations):\n",
    "            translated = manual_translations[idx]\n",
    "            print(f\"Using manual translation for segment {idx + 1}\")\n",
    "        else:\n",
    "            translated = translate_text(sentence, translation_model, openai_api_key)\n",
    "            if translation_model == \"Ollama (Local)\":\n",
    "                translated = get_ollama_response(translated)\n",
    "            translated = clean_translation(translated)\n",
    "        translated_segments.append(\n",
    "            translated\n",
    "        )  # Keep track of the cleaned translations\n",
    "\n",
    "        # Adjust the speaking rate if the translated text is significantly longer than the original.\n",
    "        if len(translated) > len(sentence) * 1.2:\n",
    "            adjusted_rate = f\"{int(rate[:-1]) - 5}%\"\n",
    "            print(\n",
    "                f\"Adjusting speaking rate to {adjusted_rate} for segment {idx+1}.\"\n",
    "            )\n",
    "            run_generate_audio_for_segment(\n",
    "                translated, segment_file, voice, adjusted_rate\n",
    "            )\n",
    "        else:\n",
    "            run_generate_audio_for_segment(translated, segment_file, voice, rate)\n",
    "\n",
    "        if not os.path.exists(segment_file) or os.path.getsize(segment_file) == 0:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Audio generation failed for sentence {idx+1}\"\n",
    "            )\n",
    "        try:\n",
    "            segment_audio = AudioSegment.from_file(segment_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Pydub failed to load segment audio: {e}. File: {segment_file}\")\n",
    "            raise\n",
    "\n",
    "        target_duration_ms = (end - start) * 1000\n",
    "        current_duration = len(segment_audio)\n",
    "        tolerance_ms = 200\n",
    "        if abs(current_duration - target_duration_ms) > tolerance_ms:\n",
    "            if current_duration < target_duration_ms:\n",
    "                silence_duration = (\n",
    "                    target_duration_ms - current_duration - cumulative_excess\n",
    "                )\n",
    "                silence_duration = max(0, silence_duration)\n",
    "                silence = AudioSegment.silent(duration=silence_duration)\n",
    "                segment_audio += silence\n",
    "                cumulative_excess = 0\n",
    "            elif current_duration > target_duration_ms:\n",
    "                segment_audio = segment_audio[:target_duration_ms]\n",
    "                cumulative_excess += current_duration - target_duration_ms\n",
    "        audio_segments.append(segment_audio)\n",
    "        # Use manual transcript if provided, otherwise, use the original sentence\n",
    "        original_sentence = (\n",
    "            manual_transcript[idx] if manual_transcript and idx < len(manual_transcript) else sentence\n",
    "        )\n",
    "        debug_entries.append(\n",
    "            f\"Segment {idx+1} (start: {start}s, end: {end}s):\\n\"\n",
    "            f\"**Original:** {original_sentence}\\n\"\n",
    "            f\"**Translated:** {translated_segments[idx]}\\n\"  # Use the cleaned translation\n",
    "            f\"**Target duration:** {target_duration_ms/1000:.2f}s, \"\n",
    "            f\"**Audio duration:** {current_duration/1000:.2f}s, \"\n",
    "            f\"**Cumulative excess:** {cumulative_excess/1000:.2f}\"\n",
    "        )\n",
    "        if progress_callback:\n",
    "            progress = (idx + 1) / total_sentences * 80\n",
    "            progress_callback(progress)\n",
    "\n",
    "    if audio_segments:\n",
    "        final_audio = sum(audio_segments)\n",
    "        final_duration = len(final_audio)\n",
    "        tolerance_ms = 500 if total_video_duration <= 600000 else 100\n",
    "        if final_duration < total_video_duration - tolerance_ms:\n",
    "            silence = AudioSegment.silent(duration=total_video_duration - final_duration)\n",
    "            final_audio += silence\n",
    "        elif final_duration > total_video_duration + tolerance_ms:\n",
    "            excess_duration = final_duration - total_video_duration\n",
    "            print(\n",
    "                \"Final audio exceeds total video duration by \"\n",
    "                f\"{excess_duration / 1000:.2f}s. Redistributing excess duration.\"\n",
    "            )\n",
    "            adjustment_ratio = excess_duration / len(audio_segments)\n",
    "            adjusted_segments = []\n",
    "            for segment in audio_segments:\n",
    "                adjusted_duration = len(segment) - adjustment_ratio\n",
    "                adjusted_segments.append(segment[: max(0, int(adjusted_duration))])\n",
    "            final_audio = sum(adjusted_segments)\n",
    "        final_audio_duration = len(final_audio)\n",
    "        if abs(final_audio_duration - total_video_duration) > 100:\n",
    "            if final_audio_duration < total_video_duration:\n",
    "                silence = AudioSegment.silent(\n",
    "                    duration=total_video_duration - final_audio_duration\n",
    "                )\n",
    "                final_audio += silence\n",
    "            elif final_audio_duration > total_video_duration:\n",
    "                final_audio = final_audio[:total_video_duration]\n",
    "        final_audio.export(FINAL_AUDIO_FILE, format=\"mp3\")\n",
    "        if not os.path.exists(FINAL_AUDIO_FILE):\n",
    "            raise RuntimeError(\"Final audio file creation failed\")\n",
    "        if abs(len(AudioSegment.from_file(FINAL_AUDIO_FILE)) - total_video_duration) > 100:\n",
    "            raise ValueError(\n",
    "                f\"Final audio duration mismatch: {len(final_audio)/1000:.1f}s vs video \"\n",
    "                f\"{total_video_duration/1000:.1f}s\"\n",
    "            )\n",
    "        debug_log_path = create_translation_log(debug_entries)\n",
    "        if not debug_log_path:\n",
    "            print(\"Debug log file could not be created.\")\n",
    "        print(\"Final synchronized audio generated!\")\n",
    "        return FINAL_AUDIO_FILE, debug_log_path\n",
    "    else:\n",
    "        print(\"No audio segments were generated.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_transcript(transcript_text: str, filename: str = \"transcript.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript_text)\n",
    "    print(f\"Transcript saved to {filename}\")\n",
    "\n",
    "def merge_audio_with_video(video_path: str, audio_path: str):\n",
    "    try:\n",
    "        print(\"Merging audio with video...\")\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = AudioFileClip(audio_path)\n",
    "        output_video_path = OUTPUT_VIDEO\n",
    "        video = video.set_audio(audio)\n",
    "        video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "        video.close()\n",
    "        audio.close()\n",
    "        if not os.path.exists(output_video_path) or os.path.getsize(output_video_path) == 0:\n",
    "            raise RuntimeError(\"Merged video file is missing or invalid.\")\n",
    "        return output_video_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to merge audio with video: {e}\")\n",
    "        raise\n",
    "\n",
    "def translate_with_openai(text: str, api_key: str, target_language: str = \"fr\") -> str:\n",
    "    try:\n",
    "        openai.api_key = api_key\n",
    "        prompt = f\"\"\"You are a professional translator specializing in ERP Cloud Fusion systems.\n",
    "        Translate the following text into {target_language}, ensuring that technical terms\n",
    "        and user interface elements are accurately translated in the context of ERP Cloud Fusion.\n",
    "\n",
    "        Only return the translated sentence without introductory phrases.\n",
    "        Do not add anything beyond the translation itself.\n",
    "\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "def validate_transcript_format(transcript: str):\n",
    "    for line in transcript.splitlines():\n",
    "        line = re.sub(r'^Texte\\s*:\\s*', '', line)\n",
    "        if not re.match(r'^\\d+:\\d+\\s*-\\s*\\d+:\\d+:\\s*.+$', line):\n",
    "            print(f\"Invalid transcript line format: {line}\")\n",
    "\n",
    "def translate_with_ollama(text: str, model: str = \"7shi/llama-translate:8b-q4_K_M\", output_file: str = \"ollama_response.txt\") -> str:\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt = (\n",
    "                f\"Translate the following text into French, ensuring technical terms and UI elements \"\n",
    "                f\"are accurately translated in the context of ERP Cloud Fusion.\\n\\n\"\n",
    "                f\"Only return the translated sentence with no extra formatting or commentary.\\n\\n\"\n",
    "                f\"Text: {text}\"\n",
    "            )\n",
    "        )\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Ollama Response:\\n\")\n",
    "            f.write(str(response))\n",
    "        if \"response\" not in response:\n",
    "            print(\"Unexpected Ollama response format.\")\n",
    "            return text\n",
    "        translated_text = response[\"response\"].strip()\n",
    "        if not translated_text:\n",
    "            print(\"Ollama returned an empty or invalid response.\")\n",
    "            return text\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama Translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Debugging Functions ---\n",
    "def create_translation_log(debug_entries: list) -> str:\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file_path = f\"translation_debug_{timestamp}.md\"\n",
    "        with open(log_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# Translation Debug Log\\n\\n\")\n",
    "            for entry in debug_entries:\n",
    "                f.write(entry + \"\\n---\\n\")\n",
    "        return log_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create debug log: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Core Functions ---\n",
    "def chunk_text(text: str, max_length: int = 1000) -> list:\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > max_length:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def translate_text(text: str, translation_model: str = \"Google Translate\", openai_api_key: str = None) -> str:\n",
    "    try:\n",
    "        if translation_model == \"OpenAI (Cloud)\":\n",
    "            print(\"Using OpenAI for translation...\")\n",
    "            return translate_with_openai(text, openai_api_key, target_language=\"fr\")\n",
    "        elif translation_model == \"Ollama (Local)\":\n",
    "            print(\"Using Ollama for translation...\")\n",
    "            return translate_with_ollama(text)\n",
    "        elif translation_model == \"Google Translate\":\n",
    "            print(\"Using Google Translate for translation...\")\n",
    "            chunks = chunk_text(text, max_length=512)\n",
    "            translated_chunks = []\n",
    "            for chunk in chunks:\n",
    "                clean_chunk = chunk.strip()\n",
    "                if not clean_chunk:\n",
    "                    continue\n",
    "                try:\n",
    "                    translated = GoogleTranslator(source='auto', target='fr').translate(clean_chunk)\n",
    "                    if not translated.strip():\n",
    "                        raise ValueError(\"Empty translation\")\n",
    "                    translated_chunks.append(translated)\n",
    "                except Exception as e:\n",
    "                    print(f\"Translation failed for chunk: {clean_chunk}. Using original text. Error: {e}\")\n",
    "                    translated_chunks.append(clean_chunk)\n",
    "            return \"\\n\".join(translated_chunks)\n",
    "        else:\n",
    "            print(f\"Unknown translation model: {translation_model}. Using original text.\")\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Translation process failed: {e}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "def parse_transcript(transcript: str):\n",
    "    sentence_groups = []\n",
    "    current_group = []\n",
    "    sentence_end_pattern = r'[.!?](?:\\s|$)'\n",
    "    base_segments = []\n",
    "    for line in transcript.splitlines():\n",
    "        line = re.sub(r'^Texte\\s*:\\s*', '', line)\n",
    "        match = re.search(r'(\\d+:\\d+)\\s*-\\s*(\\d+:\\d+):\\s*(.+)$', line)\n",
    "        if match:\n",
    "            start = convert_time(match.group(1))\n",
    "            end = convert_time(match.group(2))\n",
    "            text = match.group(3).strip()\n",
    "            base_segments.append((start, end, text))\n",
    "        else:\n",
    "            print(f\"Line skipped due to incorrect format: {line}\")\n",
    "    if not base_segments:\n",
    "        raise ValueError(\"No valid timestamped segments found in the transcript.\")\n",
    "    for seg_start, seg_end, text in base_segments:\n",
    "        current_group.append((seg_start, seg_end, text))\n",
    "        if re.search(sentence_end_pattern, text):\n",
    "            full_text = ' '.join(t for _, _, t in current_group)\n",
    "            group_start = current_group[0][0]\n",
    "            group_end = current_group[-1][1]\n",
    "            sentence_groups.append((group_start, group_end, full_text))\n",
    "            current_group = []\n",
    "    if current_group:\n",
    "        full_text = ' '.join(t for _, _, t in current_group)\n",
    "        group_start = current_group[0][0]\n",
    "        group_end = current_group[-1][1]\n",
    "        sentence_groups.append((group_start, group_end, full_text))\n",
    "    if not sentence_groups:\n",
    "        raise ValueError(\"No valid sentence groups found in the transcript.\")\n",
    "    return sentence_groups\n",
    "\n",
    "def convert_time(time_str: str) -> int:\n",
    "    m, s = map(int, time_str.split(':'))\n",
    "    return m * 60 + s\n",
    "\n",
    "def convert_seconds_to_time(seconds: int) -> str:\n",
    "    m, s = divmod(seconds, 60)\n",
    "    return f\"{m:02}:{s:02}\"\n",
    "\n",
    "\n",
    "\n",
    "def run_generate_audio_for_segment(text: str, output_file: str, voice: str, rate: str):\n",
    "    nest_asyncio.apply()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(generate_segment_audio(text, output_file, voice, rate))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate audio for segment: {e}\")\n",
    "\n",
    "def generate_transcript(video_path: str) -> str:\n",
    "    print(\"Extracting audio from video...\")\n",
    "    try:\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        audio_clip = video_clip.audio\n",
    "        audio_temp_file = \"temp_audio.wav\"\n",
    "        audio_clip.write_audiofile(audio_temp_file)\n",
    "        audio_clip.close()\n",
    "        video_clip.close()\n",
    "        print(\"Transcribing audio content...\")\n",
    "        model = whisper.load_model(\"base\")\n",
    "        result = model.transcribe(audio_temp_file)\n",
    "        os.remove(audio_temp_file)\n",
    "        transcript_lines = []\n",
    "        for segment in result[\"segments\"]:\n",
    "            start_min = int(segment[\"start\"] // 60)\n",
    "            start_sec = int(segment[\"start\"] % 60)\n",
    "            end_min = int(segment[\"end\"] // 60)\n",
    "            end_sec = int(segment[\"end\"] % 60)\n",
    "            text = segment[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "            transcript_lines.append(f\"{start_min:01d}:{start_sec:02d} - {end_min:01d}:{end_sec:02d}: {text}\")\n",
    "        return \"\\n\".join(transcript_lines)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcript generation: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c792955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed56f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
