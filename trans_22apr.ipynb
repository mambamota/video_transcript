{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fec10d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <coroutine object TranslationPipeline.process_segment at 0x000002613432C640>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object TranslationPipeline.process_segment at 0x000002613432C640>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 311\u001b[39m\n\u001b[32m    308\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Final translated video created at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ffmpeg\n",
    "import pydub\n",
    "import pysrt\n",
    "import time\n",
    "import asyncio\n",
    "import edge_tts\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
    "from faster_whisper import WhisperModel\n",
    "from shutil import which\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import aiohttp\n",
    "import ssl\n",
    "import random\n",
    "from pydub.silence import detect_nonsilent\n",
    "from typing import List, Dict\n",
    "\n",
    "# ============== Configuration ==============\n",
    "FFMPEG_PATH = which(\"ffmpeg\")\n",
    "INPUT_VIDEO = \"to translate/4.2.4_Configuration de la solution_Avr_10_Latest.mp4\"\n",
    "BASE_NAME = os.path.splitext(os.path.basename(INPUT_VIDEO))[0]\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = f\"{BASE_NAME}_run_{TIMESTAMP}\"\n",
    "MODEL_SIZE = \"small\"\n",
    "USE_EDGE_TTS = True\n",
    "\n",
    "# ============== Audio Processing Functions ==============\n",
    "class AudioProcessor:\n",
    "    @staticmethod\n",
    "    def apply_speed_adjustment(raw_audio: AudioSegment, speed_setting: str) -> AudioSegment:\n",
    "        \"\"\"Apply speed adjustment with duration compensation\"\"\"\n",
    "        speed_factor = 1 + (int(speed_setting.strip('%')) / 100)\n",
    "        original_duration = len(raw_audio)\n",
    "        \n",
    "        # Speed adjustment with crossfade to avoid clicks\n",
    "        sped_up = raw_audio.speedup(\n",
    "            playback_speed=speed_factor,\n",
    "            chunk_size=150,\n",
    "            crossfade=25\n",
    "        )\n",
    "        \n",
    "        # Calculate compensation\n",
    "        new_duration = len(sped_up)\n",
    "        compensation_ms = original_duration - new_duration\n",
    "        \n",
    "        if compensation_ms > 0:\n",
    "            return sped_up + AudioSegment.silent(duration=compensation_ms)\n",
    "        return sped_up\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_phrase_audio(text: str, voice_speed: str) -> AudioSegment:\n",
    "        \"\"\"Generate phrase audio with natural ending detection\"\"\"\n",
    "        async def _generate():\n",
    "            communicate = edge_tts.Communicate(text)\n",
    "            return await communicate\n",
    "        \n",
    "        raw_audio = asyncio.run(_generate()).audio\n",
    "        processed = AudioProcessor.apply_speed_adjustment(raw_audio, voice_speed)\n",
    "        \n",
    "        # Detect natural speech endings\n",
    "        non_silent = detect_nonsilent(\n",
    "            processed, \n",
    "            min_silence_len=50,\n",
    "            silence_thresh=processed.dBFS - 16\n",
    "        )\n",
    "        \n",
    "        if non_silent:\n",
    "            end_pad = 150  # Minimum ending padding\n",
    "            new_end = max(non_silent[-1][1] + end_pad, len(processed))\n",
    "            return processed[:new_end]\n",
    "        return processed\n",
    "\n",
    "# ============== Timing Synchronization ==============\n",
    "class SyncValidator:\n",
    "    @staticmethod\n",
    "    def validate_segment_timing(original_duration: float, translated_audio: AudioSegment) -> AudioSegment:\n",
    "        \"\"\"Ensure audio duration matches video segment duration\"\"\"\n",
    "        audio_duration = len(translated_audio) / 1000  # Convert ms to seconds\n",
    "        drift = original_duration - audio_duration\n",
    "        \n",
    "        if abs(drift) > 0.5:  # 500ms tolerance\n",
    "            compensation_ms = int(drift * 1000)\n",
    "            if compensation_ms > 0:\n",
    "                return translated_audio + AudioSegment.silent(duration=compensation_ms)\n",
    "            else:\n",
    "                return translated_audio[:compensation_ms]\n",
    "        return translated_audio\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_phrase_timings(phrases: List[str], silences: List[float]) -> List[Dict]:\n",
    "        \"\"\"Calculate precise timings for each phrase\"\"\"\n",
    "        timings = []\n",
    "        current_time = 0.0\n",
    "        \n",
    "        for i, phrase in enumerate(phrases):\n",
    "            # Generate temporary audio to measure duration\n",
    "            with tempfile.NamedTemporaryFile() as tmp:\n",
    "                asyncio.run(edge_tts.Communicate(phrase).save(tmp.name))\n",
    "                audio = AudioSegment.from_file(tmp.name)\n",
    "                duration = len(audio) / 1000  # Convert ms to seconds\n",
    "            \n",
    "            timings.append({\n",
    "                \"start\": current_time,\n",
    "                \"end\": current_time + duration,\n",
    "                \"phrase\": phrase\n",
    "            })\n",
    "            \n",
    "            # Add silence after phrase if not last element\n",
    "            if i < len(silences):\n",
    "                current_time += duration + (silences[i] / 1000)\n",
    "        \n",
    "        return timings\n",
    "\n",
    "# ============== Main Processing Pipeline ==============\n",
    "class TranslationPipeline:\n",
    "    def __init__(self):\n",
    "        self.debug_log = []\n",
    "        \n",
    "    async def process_segment(self, segment: Dict, output_path: str):\n",
    "        \"\"\"Process single video segment with sync validation\"\"\"\n",
    "        # Generate translated audio\n",
    "        translated_audio = await self._generate_translated_audio(segment)\n",
    "\n",
    "        # Validate timing\n",
    "        original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "        validated_audio = SyncValidator.validate_segment_timing(\n",
    "            original_duration, translated_audio\n",
    "        )\n",
    "\n",
    "        # Save debug information\n",
    "        self._log_segment_debug(segment, translated_audio, validated_audio)\n",
    "\n",
    "        # Export final audio\n",
    "        validated_audio.export(output_path, format=\"wav\")\n",
    "        \n",
    "        async def _generate_translated_audio(self, segment: Dict) -> AudioSegment:\n",
    "            \"\"\"Generate translated audio with proper timing\"\"\"\n",
    "            combined_audio = AudioSegment.silent(segment[\"pre_silence\"])\n",
    "        \n",
    "        for i, phrase in enumerate(segment[\"phrases\"]):\n",
    "            # Generate phrase audio\n",
    "            phrase_audio = AudioProcessor.generate_phrase_audio(\n",
    "                phrase, segment[\"speed\"]\n",
    "            )\n",
    "            \n",
    "            # Add inter-phrase silence\n",
    "            if i > 0 and i <= len(segment[\"inter_silences\"]):\n",
    "                combined_audio += AudioSegment.silent(\n",
    "                    segment[\"inter_silences\"][i-1]\n",
    "                )\n",
    "            \n",
    "            combined_audio += phrase_audio\n",
    "        \n",
    "        # Add post-silence\n",
    "        combined_audio += AudioSegment.silent(segment[\"post_silence\"])\n",
    "        \n",
    "        return combined_audio\n",
    "    \n",
    "    def _log_segment_debug(self, segment, translated_audio, validated_audio):\n",
    "      \"\"\"Log segment debug information.\"\"\"\n",
    "      # Duration in seconds\n",
    "      original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "      translated_duration = len(translated_audio) / 1000\n",
    "      validated_duration = len(validated_audio) / 1000\n",
    "\n",
    "      # Log entry\n",
    "      log_entry = {\n",
    "          \"segment_start\": segment[\"start\"],\n",
    "          \"segment_end\": segment[\"end\"],\n",
    "          \"original_duration\": original_duration,\n",
    "          \"translated_duration\": translated_duration,\n",
    "          \"validated_duration\": validated_duration,\n",
    "          \"voice_speed\": segment[\"speed\"],\n",
    "          \"pre_silence\": segment[\"pre_silence\"],\n",
    "          \"post_silence\": segment[\"post_silence\"],\n",
    "          \"inter_silences\": segment[\"inter_silences\"],\n",
    "          \"phrases\": segment[\"phrases\"],\n",
    "      }\n",
    "      self.debug_log.append(log_entry)\n",
    "\n",
    "# ============== Helper Functions ==============\n",
    "def sanitize_silences(silences: List[float]) -> List[float]:\n",
    "    \"\"\"Ensure silences are within valid range\"\"\"\n",
    "    return [max(0, min(5000, s)) for s in silences]\n",
    "\n",
    "def parse_review_file(review_path: str) -> List[Dict]:\n",
    "    \"\"\"Parse review file with sanity checks\"\"\"\n",
    "    segments = []\n",
    "    current_segment = {}\n",
    "    segment_number = 0\n",
    "\n",
    "    with open(review_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Segment\"):\n",
    "          segment_number += 1\n",
    "          # Save previous segment if it exists\n",
    "          if current_segment:\n",
    "            segments.append(current_segment)\n",
    "          # Start a new segment\n",
    "          current_segment = {}\n",
    "          match = re.match(r\"Segment (\\d+) \\(start: (\\d+\\.?\\d*)s, end: (\\d+\\.?\\d*)s\\):\", line)\n",
    "          if match:\n",
    "            current_segment['segment_number'] = int(match.group(1))\n",
    "            current_segment['start'] = float(match.group(2))\n",
    "            current_segment['end'] = float(match.group(3))\n",
    "        elif line.startswith(\"**Original:**\"):\n",
    "          current_segment['original'] = line.split('**Original:**')[1].strip()\n",
    "        elif line.startswith(\"**Auto Translated:**\"):\n",
    "          current_segment['auto_translated'] = line.split('**Auto Translated:**')[1].strip()\n",
    "        elif line.startswith(\"**Final Translation:**\"):\n",
    "          current_segment['phrases'] = split_french_phrases(line.split('**Final Translation:**')[1].strip())\n",
    "        elif line.startswith(\"**Voice Speed:**\"):\n",
    "          current_segment['speed'] = line.split('**Voice Speed:**')[1].strip()\n",
    "        elif line.startswith(\"**Pre-Silence:**\"):\n",
    "          current_segment['pre_silence'] = int(line.split('**Pre-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Post-Silence:**\"):\n",
    "          current_segment['post_silence'] = int(line.split('**Post-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Inter-Phrase-Silence:**\"):\n",
    "          silences_str = line.split('**Inter-Phrase-Silence:**')[1].strip()\n",
    "          if silences_str:\n",
    "            current_segment['inter_silences'] = [int(s.strip()) for s in silences_str.split(',') if s.strip()]\n",
    "          else:\n",
    "            current_segment['inter_silences'] = []\n",
    "        elif line.startswith(\"**DÃ©calage (local ms):**\"):\n",
    "          current_segment['decalage'] = int(line.split('**DÃ©calage (local ms):**')[1].strip())\n",
    "      if current_segment:\n",
    "        segments.append(current_segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def split_french_phrases(text: str) -> List[str]:\n",
    "    \"\"\"Splits a French text into phrases using common punctuation marks.\"\"\"\n",
    "    # Split by periods, question marks, and exclamation points, but keep the delimiters.\n",
    "    phrases = re.split(r\"([.?!])\", text)\n",
    "    # Recombine the delimiters with the preceding text.\n",
    "    # Here's the fix: convert phrases to an iterator explicitly\n",
    "    phrases_iter = iter(phrases)\n",
    "    phrases = [phrase + next(phrases_iter, '') for phrase in phrases]\n",
    "    # Clean up: remove empty strings and strip whitespace.\n",
    "    phrases = [p.strip() for p in phrases if p.strip()]\n",
    "    return phrases\n",
    "\n",
    "\n",
    "# ============== Main Execution ==============\n",
    "async def main():\n",
    "    # Initialize pipeline\n",
    "    pipeline = TranslationPipeline()\n",
    "\n",
    "    # Load review configuration\n",
    "    review_file_path = \"translation_review.txt\"\n",
    "    segments = parse_review_file(review_file_path)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = OUTPUT_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each segment\n",
    "    tasks = []\n",
    "    audio_segments = []\n",
    "    for idx, segment in enumerate(segments):\n",
    "        output_path = os.path.join(output_dir, f\"segment_{idx+1}.wav\")\n",
    "        task = pipeline.process_segment(segment, output_path)\n",
    "        tasks.append(task)\n",
    "        audio_segments.append(output_path)\n",
    "\n",
    "    # Run all tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Final video assembly\n",
    "    input_video_path = INPUT_VIDEO\n",
    "    original_video = VideoFileClip(input_video_path)\n",
    "    \n",
    "    # Load the audio segments\n",
    "    audio_clips = [AudioFileClip(audio_path) for audio_path in audio_segments]\n",
    "    \n",
    "    # Prepare video segments\n",
    "    video_segments = []\n",
    "    current_time = 0\n",
    "    for i, segment in enumerate(segments):\n",
    "        start_time = segment[\"start\"]\n",
    "        end_time = segment[\"end\"]\n",
    "        \n",
    "        # Extract the video clip for the segment\n",
    "        video_clip = original_video.subclip(start_time, end_time)\n",
    "        \n",
    "        # Set the audio of the video clip\n",
    "        video_clip = video_clip.set_audio(audio_clips[i])\n",
    "        \n",
    "        video_segments.append(video_clip)\n",
    "        current_time = end_time\n",
    "\n",
    "    # Concatenate all video segments\n",
    "    final_video = concatenate_videoclips(video_segments)\n",
    "    \n",
    "    # Set the FPS to the original video's FPS\n",
    "    final_video = final_video.set_fps(original_video.fps)\n",
    "\n",
    "    # Write the final video file\n",
    "    output_video_path = os.path.join(output_dir, \"final_translated_video.mp4\")\n",
    "    final_video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\", temp_audiofile='temp-audio.m4a', remove_temp=True)\n",
    "    \n",
    "    print(f\"âœ… Final translated video created at: {output_video_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291994da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: It seems you are trying to call asyncio.run from within an already running event loop.\n",
      "This can happen in interactive environments like Jupyter notebooks.\n",
      "Please try running this script from a regular Python environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ffmpeg\n",
    "import pydub\n",
    "import pysrt\n",
    "import time\n",
    "import asyncio\n",
    "import edge_tts\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
    "from faster_whisper import WhisperModel\n",
    "from shutil import which\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import aiohttp\n",
    "import ssl\n",
    "import random\n",
    "from pydub.silence import detect_nonsilent\n",
    "from typing import List, Dict\n",
    "\n",
    "# ============== Configuration ==============\n",
    "FFMPEG_PATH = which(\"ffmpeg\")\n",
    "INPUT_VIDEO = \"to translate/4.2.4_Configuration de la solution_Avr_10_Latest.mp4\"\n",
    "BASE_NAME = os.path.splitext(os.path.basename(INPUT_VIDEO))[0]\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = f\"{BASE_NAME}_run_{TIMESTAMP}\"\n",
    "MODEL_SIZE = \"small\"\n",
    "USE_EDGE_TTS = True\n",
    "\n",
    "# ============== Audio Processing Functions ==============\n",
    "class AudioProcessor:\n",
    "    @staticmethod\n",
    "    def apply_speed_adjustment(raw_audio: AudioSegment, speed_setting: str) -> AudioSegment:\n",
    "        \"\"\"Apply speed adjustment with duration compensation\"\"\"\n",
    "        speed_factor = 1 + (int(speed_setting.strip('%')) / 100)\n",
    "        original_duration = len(raw_audio)\n",
    "        \n",
    "        # Speed adjustment with crossfade to avoid clicks\n",
    "        sped_up = raw_audio.speedup(\n",
    "            playback_speed=speed_factor,\n",
    "            chunk_size=150,\n",
    "            crossfade=25\n",
    "        )\n",
    "        \n",
    "        # Calculate compensation\n",
    "        new_duration = len(sped_up)\n",
    "        compensation_ms = original_duration - new_duration\n",
    "        \n",
    "        if compensation_ms > 0:\n",
    "            return sped_up + AudioSegment.silent(duration=compensation_ms)\n",
    "        return sped_up\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_phrase_audio(text: str, voice_speed: str) -> AudioSegment:\n",
    "        \"\"\"Generate phrase audio with natural ending detection\"\"\"\n",
    "        async def _generate():\n",
    "            communicate = edge_tts.Communicate(text)\n",
    "            return await communicate\n",
    "        \n",
    "        raw_audio = asyncio.run(_generate()).audio\n",
    "        processed = AudioProcessor.apply_speed_adjustment(raw_audio, voice_speed)\n",
    "        \n",
    "        # Detect natural speech endings\n",
    "        non_silent = detect_nonsilent(\n",
    "            processed, \n",
    "            min_silence_len=50,\n",
    "            silence_thresh=processed.dBFS - 16\n",
    "        )\n",
    "        \n",
    "        if non_silent:\n",
    "            end_pad = 150  # Minimum ending padding\n",
    "            new_end = max(non_silent[-1][1] + end_pad, len(processed))\n",
    "            return processed[:new_end]\n",
    "        return processed\n",
    "\n",
    "# ============== Timing Synchronization ==============\n",
    "class SyncValidator:\n",
    "    @staticmethod\n",
    "    def validate_segment_timing(original_duration: float, translated_audio: AudioSegment) -> AudioSegment:\n",
    "        \"\"\"Ensure audio duration matches video segment duration\"\"\"\n",
    "        audio_duration = len(translated_audio) / 1000  # Convert ms to seconds\n",
    "        drift = original_duration - audio_duration\n",
    "        \n",
    "        if abs(drift) > 0.5:  # 500ms tolerance\n",
    "            compensation_ms = int(drift * 1000)\n",
    "            if compensation_ms > 0:\n",
    "                return translated_audio + AudioSegment.silent(duration=compensation_ms)\n",
    "            else:\n",
    "                return translated_audio[:compensation_ms]\n",
    "        return translated_audio\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_phrase_timings(phrases: List[str], silences: List[float]) -> List[Dict]:\n",
    "        \"\"\"Calculate precise timings for each phrase\"\"\"\n",
    "        timings = []\n",
    "        current_time = 0.0\n",
    "        \n",
    "        for i, phrase in enumerate(phrases):\n",
    "            # Generate temporary audio to measure duration\n",
    "            with tempfile.NamedTemporaryFile() as tmp:\n",
    "                asyncio.run(edge_tts.Communicate(phrase).save(tmp.name))\n",
    "                audio = AudioSegment.from_file(tmp.name)\n",
    "                duration = len(audio) / 1000  # Convert ms to seconds\n",
    "            \n",
    "            timings.append({\n",
    "                \"start\": current_time,\n",
    "                \"end\": current_time + duration,\n",
    "                \"phrase\": phrase\n",
    "            })\n",
    "            \n",
    "            # Add silence after phrase if not last element\n",
    "            if i < len(silences):\n",
    "                current_time += duration + (silences[i] / 1000)\n",
    "        \n",
    "        return timings\n",
    "\n",
    "# ============== Main Processing Pipeline ==============\n",
    "class TranslationPipeline:\n",
    "    def __init__(self):\n",
    "        self.debug_log = []\n",
    "        \n",
    "    async def process_segment(self, segment: Dict, output_path: str):\n",
    "        \"\"\"Process single video segment with sync validation\"\"\"\n",
    "        # Generate translated audio\n",
    "        translated_audio = await self._generate_translated_audio(segment)\n",
    "        \n",
    "        # Validate timing\n",
    "        original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "        validated_audio = SyncValidator.validate_segment_timing(\n",
    "            original_duration, translated_audio\n",
    "        )\n",
    "        \n",
    "        # Save debug information\n",
    "        self._log_segment_debug(segment, translated_audio, validated_audio)\n",
    "        \n",
    "        # Export final audio\n",
    "        validated_audio.export(output_path, format=\"wav\")\n",
    "    \n",
    "    async def _generate_translated_audio(self, segment: Dict) -> AudioSegment:\n",
    "        \"\"\"Generate translated audio with proper timing\"\"\"\n",
    "        combined_audio = AudioSegment.silent(segment[\"pre_silence\"])\n",
    "        \n",
    "        for i, phrase in enumerate(segment[\"phrases\"]):\n",
    "            # Generate phrase audio\n",
    "            phrase_audio = AudioProcessor.generate_phrase_audio(\n",
    "                phrase, segment[\"speed\"]\n",
    "            )\n",
    "            \n",
    "            # Add inter-phrase silence\n",
    "            if i > 0 and i <= len(segment[\"inter_silences\"]):\n",
    "                combined_audio += AudioSegment.silent(\n",
    "                    segment[\"inter_silences\"][i-1]\n",
    "                )\n",
    "            \n",
    "            combined_audio += phrase_audio\n",
    "        \n",
    "        # Add post-silence\n",
    "        combined_audio += AudioSegment.silent(segment[\"post_silence\"])\n",
    "        \n",
    "        return combined_audio\n",
    "    \n",
    "    def _log_segment_debug(self, segment, translated_audio, validated_audio):\n",
    "      \"\"\"Log segment debug information.\"\"\"\n",
    "      # Duration in seconds\n",
    "      original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "      translated_duration = len(translated_audio) / 1000\n",
    "      validated_duration = len(validated_audio) / 1000\n",
    "\n",
    "      # Log entry\n",
    "      log_entry = {\n",
    "          \"segment_start\": segment[\"start\"],\n",
    "          \"segment_end\": segment[\"end\"],\n",
    "          \"original_duration\": original_duration,\n",
    "          \"translated_duration\": translated_duration,\n",
    "          \"validated_duration\": validated_duration,\n",
    "          \"voice_speed\": segment[\"speed\"],\n",
    "          \"pre_silence\": segment[\"pre_silence\"],\n",
    "          \"post_silence\": segment[\"post_silence\"],\n",
    "          \"inter_silences\": segment[\"inter_silences\"],\n",
    "          \"phrases\": segment[\"phrases\"],\n",
    "      }\n",
    "      self.debug_log.append(log_entry)\n",
    "\n",
    "# ============== Helper Functions ==============\n",
    "def sanitize_silences(silences: List[float]) -> List[float]:\n",
    "    \"\"\"Ensure silences are within valid range\"\"\"\n",
    "    return [max(0, min(5000, s)) for s in silences]\n",
    "\n",
    "def parse_review_file(review_path: str) -> List[Dict]:\n",
    "    \"\"\"Parse review file with sanity checks\"\"\"\n",
    "    segments = []\n",
    "    current_segment = {}\n",
    "    segment_number = 0\n",
    "\n",
    "    with open(review_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Segment\"):\n",
    "          segment_number += 1\n",
    "          # Save previous segment if it exists\n",
    "          if current_segment:\n",
    "            segments.append(current_segment)\n",
    "          # Start a new segment\n",
    "          current_segment = {}\n",
    "          match = re.match(r\"Segment (\\d+) \\(start: (\\d+\\.?\\d*)s, end: (\\d+\\.?\\d*)s\\):\", line)\n",
    "          if match:\n",
    "            current_segment['segment_number'] = int(match.group(1))\n",
    "            current_segment['start'] = float(match.group(2))\n",
    "            current_segment['end'] = float(match.group(3))\n",
    "        elif line.startswith(\"**Original:**\"):\n",
    "          current_segment['original'] = line.split('**Original:**')[1].strip()\n",
    "        elif line.startswith(\"**Auto Translated:**\"):\n",
    "          current_segment['auto_translated'] = line.split('**Auto Translated:**')[1].strip()\n",
    "        elif line.startswith(\"**Final Translation:**\"):\n",
    "          current_segment['phrases'] = split_french_phrases(line.split('**Final Translation:**')[1].strip())\n",
    "        elif line.startswith(\"**Voice Speed:**\"):\n",
    "          current_segment['speed'] = line.split('**Voice Speed:**')[1].strip()\n",
    "        elif line.startswith(\"**Pre-Silence:**\"):\n",
    "          current_segment['pre_silence'] = int(line.split('**Pre-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Post-Silence:**\"):\n",
    "          current_segment['post_silence'] = int(line.split('**Post-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Inter-Phrase-Silence:**\"):\n",
    "          silences_str = line.split('**Inter-Phrase-Silence:**')[1].strip()\n",
    "          if silences_str:\n",
    "            current_segment['inter_silences'] = [int(s.strip()) for s in silences_str.split(',') if s.strip()]\n",
    "          else:\n",
    "            current_segment['inter_silences'] = []\n",
    "        elif line.startswith(\"**DÃ©calage (local ms):**\"):\n",
    "          current_segment['decalage'] = int(line.split('**DÃ©calage (local ms):**')[1].strip())\n",
    "      if current_segment:\n",
    "        segments.append(current_segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def split_french_phrases(text: str) -> List[str]:\n",
    "    \"\"\"Splits a French text into phrases using common punctuation marks.\"\"\"\n",
    "    # Split by periods, question marks, and exclamation points, but keep the delimiters.\n",
    "    phrases = re.split(r\"([.?!])\", text)\n",
    "    # Recombine the delimiters with the preceding text.\n",
    "    # Here's the fix: convert phrases to an iterator explicitly\n",
    "    phrases_iter = iter(phrases)\n",
    "    phrases = [phrase + next(phrases_iter, '') for phrase in phrases]\n",
    "    # Clean up: remove empty strings and strip whitespace.\n",
    "    phrases = [p.strip() for p in phrases if p.strip()]\n",
    "    return phrases\n",
    "\n",
    "# ============== Main Execution ==============\n",
    "async def main():\n",
    "    # Initialize pipeline\n",
    "    pipeline = TranslationPipeline()\n",
    "\n",
    "    # Load review configuration\n",
    "    review_file_path = \"translation_review.txt\"\n",
    "    segments = parse_review_file(review_file_path)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = OUTPUT_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each segment\n",
    "    tasks = []\n",
    "    audio_segments = []\n",
    "    for idx, segment in enumerate(segments):\n",
    "        output_path = os.path.join(output_dir, f\"segment_{idx+1}.wav\")\n",
    "        task = pipeline.process_segment(segment, output_path)\n",
    "        tasks.append(task)\n",
    "        audio_segments.append(output_path)\n",
    "\n",
    "    # Run all tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Final video assembly\n",
    "    input_video_path = INPUT_VIDEO\n",
    "    original_video = VideoFileClip(input_video_path)\n",
    "    \n",
    "    # Load the audio segments\n",
    "    audio_clips = [AudioFileClip(audio_path) for audio_path in audio_segments]\n",
    "    \n",
    "    # Prepare video segments\n",
    "    video_segments = []\n",
    "    current_time = 0\n",
    "    for i, segment in enumerate(segments):\n",
    "        start_time = segment[\"start\"]\n",
    "        end_time = segment[\"end\"]\n",
    "        \n",
    "        # Extract the video clip for the segment\n",
    "        video_clip = original_video.subclip(start_time, end_time)\n",
    "        \n",
    "        # Set the audio of the video clip\n",
    "        video_clip = video_clip.set_audio(audio_clips[i])\n",
    "        \n",
    "        video_segments.append(video_clip)\n",
    "        current_time = end_time\n",
    "\n",
    "    # Concatenate all video segments\n",
    "    final_video = concatenate_videoclips(video_segments)\n",
    "    \n",
    "    # Set the FPS to the original video's FPS\n",
    "    final_video = final_video.set_fps(original_video.fps)\n",
    "\n",
    "    # Write the final video file\n",
    "    output_video_path = os.path.join(output_dir, \"final_translated_video.mp4\")\n",
    "    final_video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\", temp_audiofile='temp-audio.m4a', remove_temp=True)\n",
    "    \n",
    "    print(f\"âœ… Final translated video created at: {output_video_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except RuntimeError as e:\n",
    "        if \"cannot be called from a running event loop\" in str(e):\n",
    "            print(\"Error: It seems you are trying to call asyncio.run from within an already running event loop.\")\n",
    "            print(\"This can happen in interactive environments like Jupyter notebooks.\")\n",
    "            print(\"Please try running this script from a regular Python environment.\")\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af07bad",
   "metadata": {},
   "source": [
    "23APR25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307134f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] TTS attempt 1 failed for phrase: Nous allons voir les configurations de l'application EPM.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE96BA0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Nous allons voir les configurations de l'application EPM.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE969F0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous verrons comment la sÃ©curitÃ© fonctionne dans l'application EPM et nous couvrirons comment crÃ©er et configurer des formulaires de donnÃ©es.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE96A80> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: la sÃ©curitÃ© par tÃ¢ches ou le flux de travail, la sÃ©curitÃ© des rÃ¨gles mÃ©tier et la sÃ©curitÃ© des donnÃ©es.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE96180> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: la sÃ©curitÃ© par tÃ¢ches ou le flux de travail, la sÃ©curitÃ© des rÃ¨gles mÃ©tier et la sÃ©curitÃ© des donnÃ©es.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE976E0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 413\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# Nous pouvons maintenant utiliser run_until_complete mÃªme si un loop tourne dÃ©jÃ \u001b[39;00m\n\u001b[32m    412\u001b[39m     loop = asyncio.get_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AI PROJECTS\\video_transcript\\myenv\\Lib\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AI PROJECTS\\video_transcript\\myenv\\Lib\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = select.select(r, w, w, timeout)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] TTS attempt 1 failed for phrase: Ici, le matÃ©riel OFS sera divisÃ© par le volume OFS.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702210> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais annuler dans ce cas.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702B10> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Pour voir une rÃ¨gle mÃ©tier, cliquer sur l'icÃ´ne Navigateur et cliquer sur RÃ¨gles.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA15ED370> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Pour voir une rÃ¨gle mÃ©tier, cliquer sur l'icÃ´ne Navigateur et cliquer sur RÃ¨gles.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701880> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je peux importer et je peux voir les propriÃ©tÃ©s.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702570> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Comme j'ai sÃ©lectionnÃ© le cube et l'application, toutes ces informations sont prÃ©populÃ©es.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17027B0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Le systÃ¨me indiquera que la rÃ¨gle a Ã©tÃ© crÃ©Ã© et qu'on peut y apporter des modifications.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17029F0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais sÃ©lectionner la vue de script.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701D00> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous avons toutes les dimensions requises dont nous avons besoin ici.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701AC0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Nous avons toutes les dimensions requises dont nous avons besoin ici.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701B50> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: La rÃ¨gle va copier les donnÃ©es de FY24 pour le scÃ©nario plan, working, de la contrepartie de l'allocation d'entitÃ©, au plan FY23, working, contrepartie de l'allocation d'entitÃ©.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702CC0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Pour copier des donnÃ©es, nous devons simplement exÃ©cuter la rÃ¨gle mÃ©tier et il copiera les donnÃ©es de FY24 Ã  FY23.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701F40> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Pour copier des donnÃ©es, nous devons simplement exÃ©cuter la rÃ¨gle mÃ©tier et il copiera les donnÃ©es de FY24 Ã  FY23.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702210> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 3 failed for phrase: Pour copier des donnÃ©es, nous devons simplement exÃ©cuter la rÃ¨gle mÃ©tier et il copiera les donnÃ©es de FY24 Ã  FY23.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702840> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Error] All TTS attempts failed; inserted 500ms silence at C:\\Users\\061181~1\\AppData\\Local\\Temp\\tmp_7_0.mp3\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous pouvons Ã©galement cÃ©duler la rÃ¨gle mÃ©tier dans nos automations et gÃ©rer dans le cadre des travaux quotidiens d'automatisation.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17022A0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Regardons la sÃ©curitÃ© dimensionnelle.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA15ED370> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais aller dans le dossier DÃ©mo et j'ai un formulaire et en cliquant sur ceci, je vais ajouter un groupe.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701C70> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Voyons comment crÃ©er et configurer un formulaire de donnÃ©es. Ã€ partir de la page d'accueil cliquez sur la carte DonnÃ©es.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17029F0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous devons entrer le nom.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702960> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Cliquer sur l'icÃ´ne me donnera le sÃ©lecteur de membres et je peux ajouter le membre requis.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702690> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Cliquer sur l'icÃ´ne me donnera le sÃ©lecteur de membres et je peux ajouter le membre requis.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17020F0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: C'est lÃ  que nous pouvons ajouter une rÃ¨gle mÃ©tier que nous avons crÃ©Ã©e pour cette application.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701C70> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous cliquerons sur OK.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702180> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais cliquer sur OK.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE97770> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais les enregistrer.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17017F0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je veux Ã©galement reflÃ©ter ces donnÃ©es pour tous les autres chiffres.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE97770> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Est-ce un ajustement proportionnel ou un ajustement uniforme?\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17023C0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous pouvons dÃ©finir la langue et les zones.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702B10> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous dÃ©finirons les variables utilisateur Ã  partir d'ici en sÃ©lectionnant le sÃ©lecteur de membre et en sÃ©lectionnant le membre respectif et en cliquant sur OK, puis en l'enregistrant.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701BE0> [Une connexion existante a dÃ» Ãªtre fermÃ©e par lâ€™hÃ´te distant]\n",
      "Moviepy - Building video 4.2.4_Configuration de la solution_Avr_10_Latest_run_20250423_112245\\4.2.4_Configuration de la solution_Avr_10_Latest-french.mp4.\n",
      "MoviePy - Writing audio in temp-audio.m4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video 4.2.4_Configuration de la solution_Avr_10_Latest_run_20250423_112245\\4.2.4_Configuration de la solution_Avr_10_Latest-french.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready 4.2.4_Configuration de la solution_Avr_10_Latest_run_20250423_112245\\4.2.4_Configuration de la solution_Avr_10_Latest-french.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ffmpeg\n",
    "import pysrt\n",
    "import tempfile\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "from faster_whisper import WhisperModel\n",
    "from shutil import which\n",
    "import edge_tts\n",
    "import aiohttp\n",
    "import random\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ffmpeg_path = which(\"ffmpeg\")\n",
    "if not ffmpeg_path:\n",
    "    raise RuntimeError(\"ffmpeg not found. Please install ffmpeg first.\")\n",
    "\n",
    "input_video     = \"to translate/4.2.4_Configuration de la solution_Avr_10_Latest.mp4\"\n",
    "base_name       = os.path.splitext(os.path.basename(input_video))[0]\n",
    "timestamp       = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir      = f\"{base_name}_run_{timestamp}\"\n",
    "model_size      = \"small\"\n",
    "USE_EDGE_TTS    = True\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "extracted_audio = os.path.join(output_dir, f\"{base_name}-extracted-audio.wav\")\n",
    "subtitle_file   = os.path.join(output_dir, f\"{base_name}-english.srt\")\n",
    "review_file     = os.path.join(output_dir, \"translation_review.txt\")\n",
    "debug_log_file  = os.path.join(output_dir, \"translation_debug_log.txt\")\n",
    "translated_audio= os.path.join(output_dir, f\"{base_name}-french.wav\")\n",
    "output_video    = os.path.join(output_dir, f\"{base_name}-french.mp4\")\n",
    "\n",
    "# â”€â”€ Helpers: audio extraction & whisper transcription â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def extract_audio():\n",
    "    ffmpeg.input(input_video).output(extracted_audio, ac=1, ar=16000) \\\n",
    "        .overwrite_output().run(capture_stdout=True, capture_stderr=True)\n",
    "    return extracted_audio\n",
    "\n",
    "def transcribe(path):\n",
    "    model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "    segments, info = model.transcribe(path, beam_size=5)\n",
    "    return info.language, [\n",
    "        {\"start\": seg.start, \"end\": seg.end, \"text\": seg.text.strip()}\n",
    "        for seg in segments\n",
    "    ]\n",
    "\n",
    "def time_to_subrip(seconds: float) -> pysrt.SubRipTime:\n",
    "    h = int(seconds // 3600)\n",
    "    seconds %= 3600\n",
    "    m = int(seconds // 60)\n",
    "    seconds %= 60\n",
    "    ms = int((seconds - int(seconds)) * 1000)\n",
    "    return pysrt.SubRipTime(hours=h, minutes=m, seconds=int(seconds), milliseconds=ms)\n",
    "\n",
    "def generate_subtitle_file(segments, outpath):\n",
    "    subs = pysrt.SubRipFile()\n",
    "    for i, seg in enumerate(segments, 1):\n",
    "        subs.append(pysrt.SubRipItem(\n",
    "            index=i,\n",
    "            start=time_to_subrip(seg[\"start\"]),\n",
    "            end=time_to_subrip(seg[\"end\"]),\n",
    "            text=seg[\"text\"]\n",
    "        ))\n",
    "    subs.save(outpath, encoding=\"utf-8\")\n",
    "    return outpath\n",
    "\n",
    "# â”€â”€ Text grouping & review file generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def split_long_groups(groups, max_dur):\n",
    "    new = []\n",
    "    for grp in groups:\n",
    "        start = grp[0].start.ordinal/1000\n",
    "        end   = grp[-1].end.ordinal/1000\n",
    "        if end - start <= max_dur:\n",
    "            new.append(grp)\n",
    "            continue\n",
    "        temp, ts = [], start\n",
    "        last_safe = None\n",
    "        for idx, item in enumerate(grp):\n",
    "            temp.append(item)\n",
    "            if re.search(r\"[.,!?]$\", item.text.strip()):\n",
    "                last_safe = idx\n",
    "            cur_end = item.end.ordinal/1000\n",
    "            if cur_end - ts >= max_dur:\n",
    "                if last_safe is not None:\n",
    "                    new.append(temp[:last_safe+1])\n",
    "                    temp = temp[last_safe+1:]\n",
    "                    ts   = temp[0].start.ordinal/1000 if temp else cur_end\n",
    "                else:\n",
    "                    new.append(temp)\n",
    "                    temp = []\n",
    "                    ts   = cur_end\n",
    "                last_safe = None\n",
    "        if temp:\n",
    "            new.append(temp)\n",
    "    return new\n",
    "\n",
    "def enforce_punctuation_boundaries(groups):\n",
    "    i = 0\n",
    "    safe = r\"[.!?,;:]$\"\n",
    "    while i < len(groups):\n",
    "        last = groups[i][-1].text.strip()\n",
    "        if not re.search(safe, last):\n",
    "            if i+1 < len(groups):\n",
    "                groups[i] += groups.pop(i+1)\n",
    "            else:\n",
    "                groups[i][-1].text += \".\"\n",
    "        else:\n",
    "            i += 1\n",
    "    return groups\n",
    "\n",
    "def generate_translation_review_file(source_path, review_path,\n",
    "                                     from_lang=\"en\", to_lang=\"fr\",\n",
    "                                     max_group_duration_secs=25.0):\n",
    "    translator = GoogleTranslator(source=from_lang, target=to_lang)\n",
    "    subs       = pysrt.open(source_path)\n",
    "    sentence_end = re.compile(r\"[.!?]\\s*$\")\n",
    "\n",
    "    # 1) group subs by sentence\n",
    "    groups, cur = [], []\n",
    "    for sub in subs:\n",
    "        cur.append(sub)\n",
    "        if sentence_end.search(sub.text):\n",
    "            groups.append(cur)\n",
    "            cur = []\n",
    "    if cur: groups.append(cur)\n",
    "\n",
    "    # 2) split long groups, enforce punctuation\n",
    "    groups = split_long_groups(groups, max_group_duration_secs)\n",
    "    groups = enforce_punctuation_boundaries(groups)\n",
    "\n",
    "    # 3) write review file\n",
    "    with open(review_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Translation Review File\\n\")\n",
    "        f.write(\"Edit **Final Translation**, **Voice Speed**, **Preâ€‘Silence**, **Postâ€‘Silence**, **Interâ€‘Phraseâ€‘Silence**\\n\")\n",
    "        f.write(\"----------------------------------------------------------------------------- \\n\\n\")\n",
    "        for idx, grp in enumerate(groups, 1):\n",
    "            start = grp[0].start.ordinal/1000\n",
    "            end   = grp[-1].end.ordinal/1000\n",
    "            orig  = \" \".join(s.text for s in grp)\n",
    "            auto  = translator.translate(text=orig)\n",
    "            # default values\n",
    "            phrases = split_french_phrases(auto)\n",
    "            inter_default = \",\".join(\"0\" for _ in range(len(phrases)-1))\n",
    "            f.write(f\"Segment {idx} (start: {start:.2f}s, end: {end:.2f}s):\\n\")\n",
    "            f.write(f\"**Original:** {orig}\\n\")\n",
    "            f.write(f\"**Auto Translated:** {auto}\\n\")\n",
    "            f.write(f\"**Final Translation:** {auto}\\n\")\n",
    "            f.write(f\"**Voice Speed:** +0%\\n\")\n",
    "            f.write(f\"**Pre-Silence:** 100\\n\")\n",
    "            f.write(f\"**Post-Silence:** 100\\n\")\n",
    "            f.write(f\"**Inter-Phrase-Silence:** {inter_default}\\n\")\n",
    "            f.write(\"-----------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "    input(\"Review file ready. Press Enter to continueâ€¦\")\n",
    "    return groups\n",
    "\n",
    "# â”€â”€ Review overrides parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def parse_review_overrides(review_file_path):\n",
    "    text   = open(review_file_path, \"r\", encoding=\"utf-8\").read()\n",
    "    blocks = re.split(r\"(?m)^-{3,}\\s*$\", text)\n",
    "    overrides = []\n",
    "    for idx, blk in enumerate(blocks, start=1):\n",
    "        blk = blk.strip()\n",
    "        if not blk or blk.startswith(\"Translation Review File\"):\n",
    "            continue\n",
    "        # defaults\n",
    "        ft       = None\n",
    "        vs       = \"+0%\"\n",
    "        pre_ms   = 0.0\n",
    "        post_ms  = 100.0\n",
    "        inter_ms = []\n",
    "        for line in blk.splitlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"**Final Translation:**\"):\n",
    "                ft = line.split(\"**Final Translation:**\",1)[1].strip()\n",
    "            elif line.startswith(\"**Voice Speed:**\"):\n",
    "                vs = line.split(\"**Voice Speed:**\",1)[1].strip()\n",
    "            elif line.startswith(\"**Pre-Silence:**\"):\n",
    "                try: pre_ms = float(line.split(\"**Pre-Silence:**\",1)[1].strip())\n",
    "                except: pass\n",
    "            elif line.startswith(\"**Post-Silence:**\"):\n",
    "                try: post_ms = float(line.split(\"**Post-Silence:**\",1)[1].strip())\n",
    "                except: pass\n",
    "            elif line.startswith(\"**Inter-Phrase-Silence:**\"):\n",
    "                parts = line.split(\"**Inter-Phrase-Silence:**\",1)[1].strip()\n",
    "                if parts:\n",
    "                    try:\n",
    "                        inter_ms = [float(x) for x in parts.split(\",\")]\n",
    "                    except: pass\n",
    "        overrides.append({\n",
    "            \"final_translation\":     ft,\n",
    "            \"voice_speed\":           vs,\n",
    "            \"pre_silence\":           pre_ms,\n",
    "            \"post_silence\":          post_ms,\n",
    "            \"inter_phrase_silences\": inter_ms\n",
    "        })\n",
    "    return overrides\n",
    "\n",
    "# â”€â”€ Phrase splitting & weighting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def split_french_phrases(text):\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+(?=[A-Z])\", text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def calculate_phrase_weights(orig, phrases):\n",
    "    counts = [len(p.split()) for p in phrases]\n",
    "    total  = sum(counts)\n",
    "    if total == 0:\n",
    "        return [1/len(phrases)]*len(phrases)\n",
    "    return [c/total for c in counts]\n",
    "\n",
    "# â”€â”€ Audio adjustments & TTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def adjust_audio_duration(audio, target_dur):\n",
    "    cur = audio.duration_seconds\n",
    "    diff= target_dur - cur\n",
    "    if diff > 0.1:\n",
    "        return audio + AudioSegment.silent(duration=diff*1000)\n",
    "    return audio\n",
    "\n",
    "def change_playback_speed(sound, speed=1.0):\n",
    "    new_rate = int(sound.frame_rate * speed)\n",
    "    altered  = sound._spawn(sound.raw_data, overrides={\"frame_rate\": new_rate})\n",
    "    return altered.set_frame_rate(sound.frame_rate)\n",
    "\n",
    "async def robust_synthesize_phrase(\n",
    "    phrase: str,\n",
    "    outpath: str,\n",
    "    voice: str = \"fr-FR-DeniseNeural\",\n",
    "    rate: str = \"+0%\",\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession(\n",
    "                timeout=aiohttp.ClientTimeout(total=30)\n",
    "            ) as session:\n",
    "                communicate = edge_tts.Communicate(\n",
    "                    text=phrase,\n",
    "                    voice=voice,\n",
    "                    rate=rate\n",
    "                )\n",
    "                await communicate.save(outpath)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] TTS attempt {attempt+1} failed for phrase: {phrase}\\n{e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    # Si aprÃ¨s tous les essais Ã§a Ã©choue, on Ã©crit un silence de secours\n",
    "    silent = AudioSegment.silent(duration=500)  # 0.5s de silence\n",
    "    silent.export(outpath, format=\"mp3\")\n",
    "    print(f\"[Error] All TTS attempts failed; inserted 500ms silence at {outpath}\")\n",
    "\n",
    "\n",
    "\n",
    "async def synthesize_phrase_edge_hybrid(phrase, outpath, voice=\"fr-FR-DeniseNeural\", rate=\"+0%\"):\n",
    "    await robust_synthesize_phrase(phrase, outpath, voice, rate)\n",
    "\n",
    "# â”€â”€ Main async with updated generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def async_generate_translated_audio_with_sync_using_review(\n",
    "    subtitle_source_path, output_audio_path,\n",
    "    debug_log_path, review_file_path\n",
    "):\n",
    "    groups    = generate_translation_review_file(subtitle_source_path, review_file_path)\n",
    "    overrides = parse_review_overrides(review_file_path)\n",
    "\n",
    "    default_ov = {\n",
    "        \"final_translation\":     None,\n",
    "        \"voice_speed\":           \"+0%\",\n",
    "        \"pre_silence\":           0.0,\n",
    "        \"post_silence\":          100.0,\n",
    "        \"inter_phrase_silences\": []\n",
    "    }\n",
    "    while len(overrides) < len(groups):\n",
    "        overrides.append(default_ov.copy())\n",
    "\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "    debug    = []\n",
    "\n",
    "    for idx, group in enumerate(groups):\n",
    "        start_s, end_s = group[0].start.ordinal/1000, group[-1].end.ordinal/1000\n",
    "        seg_dur  = end_s - start_s\n",
    "        total_ms = int(seg_dur * 1000)\n",
    "\n",
    "        ovr     = overrides[idx]\n",
    "        text    = ovr[\"final_translation\"] or \" \".join(s.text for s in group)\n",
    "        rate    = ovr[\"voice_speed\"]\n",
    "        pre_ms  = ovr[\"pre_silence\"]\n",
    "        post_ms = ovr[\"post_silence\"]\n",
    "        inter   = ovr[\"inter_phrase_silences\"]\n",
    "\n",
    "        phrases    = split_french_phrases(text)\n",
    "        weights    = calculate_phrase_weights(text, phrases)\n",
    "        content_ms = max(0, total_ms - pre_ms - post_ms)\n",
    "\n",
    "        # synth phrases\n",
    "        phrase_audios = []\n",
    "        for i, ph in enumerate(phrases):\n",
    "            dur = content_ms * weights[i] / 1000.0\n",
    "            tmp = os.path.join(tempfile.gettempdir(), f\"tmp_{idx}_{i}.mp3\")\n",
    "            await synthesize_phrase_edge_hybrid(ph, tmp, voice=\"fr-FR-DeniseNeural\", rate=rate)\n",
    "            aud = AudioSegment.from_mp3(tmp)\n",
    "            os.remove(tmp)\n",
    "            aud = adjust_audio_duration(aud, dur)\n",
    "            phrase_audios.append(aud)\n",
    "\n",
    "        # TTS overâ€‘run protection\n",
    "        sum_tts = sum(a.duration_seconds*1000 for a in phrase_audios)\n",
    "        if sum_tts > content_ms and sum_tts > 0:\n",
    "            factor_audio = content_ms / sum_tts\n",
    "            phrase_audios = [change_playback_speed(aud, factor_audio) for aud in phrase_audios]\n",
    "            sum_tts = sum(a.duration_seconds*1000 for a in phrase_audios)\n",
    "\n",
    "        # interâ€‘phrase autoâ€‘fit (10% margin)\n",
    "        available   = total_ms - pre_ms - post_ms - sum_tts\n",
    "        total_inter = sum(inter)\n",
    "        margin      = 0.9\n",
    "        if total_inter > 0 and total_inter > available * margin:\n",
    "            factor = (available * margin) / total_inter\n",
    "            inter  = [int(ms * factor) for ms in inter]\n",
    "\n",
    "        # rebuild with silences\n",
    "        seq = []\n",
    "        for i, aud in enumerate(phrase_audios):\n",
    "            seq.append(aud)\n",
    "            if i < len(inter):\n",
    "                seq.append(AudioSegment.silent(duration=inter[i]))\n",
    "\n",
    "        # assemble segment\n",
    "        seg = AudioSegment.silent(duration=pre_ms)\n",
    "        for clip in seq:\n",
    "            seg += clip\n",
    "        seg += AudioSegment.silent(duration=post_ms)\n",
    "\n",
    "        # strip leading TTS silence\n",
    "        non = detect_nonsilent(seg, min_silence_len=1, silence_thresh=seg.dBFS-16)\n",
    "        if non:\n",
    "            seg = seg[non[0][0]:]\n",
    "        seg = AudioSegment.silent(duration=pre_ms) + seg\n",
    "\n",
    "        # pad/trim\n",
    "        if len(seg) < total_ms:\n",
    "            seg += AudioSegment.silent(duration=(total_ms - len(seg)))\n",
    "        seg = seg[:total_ms]\n",
    "\n",
    "        # measure decalage\n",
    "        non2            = detect_nonsilent(seg, min_silence_len=1, silence_thresh=seg.dBFS-16)\n",
    "        start_audio_ms  = non2[0][0] if non2 else pre_ms\n",
    "        end_audio_ms    = non2[-1][1] if non2 else total_ms - post_ms\n",
    "        abs_start_audio = int(start_s*1000) + start_audio_ms\n",
    "        abs_end_audio   = int(start_s*1000) + end_audio_ms\n",
    "        abs_start_video = int(start_s*1000)\n",
    "        abs_end_video   = int(end_s*1000)\n",
    "        decal_start = abs_start_audio - abs_start_video\n",
    "        decal_end   = abs_end_audio   - abs_end_video\n",
    "\n",
    "        # optional warp\n",
    "        gen_dur = seg.duration_seconds\n",
    "        diff    = seg_dur - gen_dur\n",
    "        if abs(diff) > 0.20:\n",
    "            seg = change_playback_speed(seg, seg_dur/gen_dur)\n",
    "\n",
    "        # place on combined\n",
    "        start_ms = int(start_s*1000)\n",
    "        if len(combined) < start_ms:\n",
    "            combined += AudioSegment.silent(duration=(start_ms-len(combined)))\n",
    "        combined += seg\n",
    "\n",
    "        debug.append(\n",
    "            f\"Segment {idx+1} ({start_s:.2f}-{end_s:.2f}s): \"\n",
    "            f\"pre={pre_ms}ms, post={post_ms}ms, speed={rate}, \"\n",
    "            f\"dÃ©calage_start={decal_start}ms, dÃ©calage_end={decal_end}ms, \"\n",
    "            f\"inter={inter}, phrases={phrases}\\n\"\n",
    "        )\n",
    "\n",
    "    # write debug & export\n",
    "    with open(debug_log_file, \"w\", encoding=\"utf-8\") as df:\n",
    "        df.write(\"Translation Debug Log\\n\\n\")\n",
    "        df.writelines(debug)\n",
    "    combined.export(output_audio_path, format=\"wav\")\n",
    "    return output_audio_path\n",
    "\n",
    "# â”€â”€ Merge & Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def merge_audio_video():\n",
    "    video = VideoFileClip(input_video)\n",
    "    audio = AudioFileClip(translated_audio)\n",
    "    if audio.duration < video.duration:\n",
    "        extra = AudioSegment.silent(duration=(video.duration - audio.duration)*1000)\n",
    "        tmpf  = os.path.join(output_dir, \"temp_full.wav\")\n",
    "        AudioSegment.from_file(translated_audio).append(extra).export(tmpf, format=\"wav\")\n",
    "        audio = AudioFileClip(tmpf)\n",
    "    video.set_audio(audio).write_videofile(output_video,\n",
    "                                            codec=\"libx264\", audio_codec=\"aac\",\n",
    "                                            temp_audiofile=\"temp-audio.m4a\", remove_temp=True)\n",
    "\n",
    "async def async_main():\n",
    "    extract_audio()\n",
    "    _, segs = transcribe(extracted_audio)\n",
    "    generate_subtitle_file(segs, subtitle_file)\n",
    "    await async_generate_translated_audio_with_sync_using_review(\n",
    "        subtitle_file, translated_audio, debug_log_file, review_file\n",
    "    )\n",
    "    merge_audio_video()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Nous pouvons maintenant utiliser run_until_complete mÃªme si un loop tourne dÃ©jÃ \n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(async_main())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
