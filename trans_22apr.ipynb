{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fec10d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <coroutine object TranslationPipeline.process_segment at 0x000002613432C640>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object TranslationPipeline.process_segment at 0x000002613432C640>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 311\u001b[39m\n\u001b[32m    308\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Final translated video created at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ffmpeg\n",
    "import pydub\n",
    "import pysrt\n",
    "import time\n",
    "import asyncio\n",
    "import edge_tts\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
    "from faster_whisper import WhisperModel\n",
    "from shutil import which\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import aiohttp\n",
    "import ssl\n",
    "import random\n",
    "from pydub.silence import detect_nonsilent\n",
    "from typing import List, Dict\n",
    "\n",
    "# ============== Configuration ==============\n",
    "FFMPEG_PATH = which(\"ffmpeg\")\n",
    "INPUT_VIDEO = \"to translate/4.2.4_Configuration de la solution_Avr_10_Latest.mp4\"\n",
    "BASE_NAME = os.path.splitext(os.path.basename(INPUT_VIDEO))[0]\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = f\"{BASE_NAME}_run_{TIMESTAMP}\"\n",
    "MODEL_SIZE = \"small\"\n",
    "USE_EDGE_TTS = True\n",
    "\n",
    "# ============== Audio Processing Functions ==============\n",
    "class AudioProcessor:\n",
    "    @staticmethod\n",
    "    def apply_speed_adjustment(raw_audio: AudioSegment, speed_setting: str) -> AudioSegment:\n",
    "        \"\"\"Apply speed adjustment with duration compensation\"\"\"\n",
    "        speed_factor = 1 + (int(speed_setting.strip('%')) / 100)\n",
    "        original_duration = len(raw_audio)\n",
    "        \n",
    "        # Speed adjustment with crossfade to avoid clicks\n",
    "        sped_up = raw_audio.speedup(\n",
    "            playback_speed=speed_factor,\n",
    "            chunk_size=150,\n",
    "            crossfade=25\n",
    "        )\n",
    "        \n",
    "        # Calculate compensation\n",
    "        new_duration = len(sped_up)\n",
    "        compensation_ms = original_duration - new_duration\n",
    "        \n",
    "        if compensation_ms > 0:\n",
    "            return sped_up + AudioSegment.silent(duration=compensation_ms)\n",
    "        return sped_up\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_phrase_audio(text: str, voice_speed: str) -> AudioSegment:\n",
    "        \"\"\"Generate phrase audio with natural ending detection\"\"\"\n",
    "        async def _generate():\n",
    "            communicate = edge_tts.Communicate(text)\n",
    "            return await communicate\n",
    "        \n",
    "        raw_audio = asyncio.run(_generate()).audio\n",
    "        processed = AudioProcessor.apply_speed_adjustment(raw_audio, voice_speed)\n",
    "        \n",
    "        # Detect natural speech endings\n",
    "        non_silent = detect_nonsilent(\n",
    "            processed, \n",
    "            min_silence_len=50,\n",
    "            silence_thresh=processed.dBFS - 16\n",
    "        )\n",
    "        \n",
    "        if non_silent:\n",
    "            end_pad = 150  # Minimum ending padding\n",
    "            new_end = max(non_silent[-1][1] + end_pad, len(processed))\n",
    "            return processed[:new_end]\n",
    "        return processed\n",
    "\n",
    "# ============== Timing Synchronization ==============\n",
    "class SyncValidator:\n",
    "    @staticmethod\n",
    "    def validate_segment_timing(original_duration: float, translated_audio: AudioSegment) -> AudioSegment:\n",
    "        \"\"\"Ensure audio duration matches video segment duration\"\"\"\n",
    "        audio_duration = len(translated_audio) / 1000  # Convert ms to seconds\n",
    "        drift = original_duration - audio_duration\n",
    "        \n",
    "        if abs(drift) > 0.5:  # 500ms tolerance\n",
    "            compensation_ms = int(drift * 1000)\n",
    "            if compensation_ms > 0:\n",
    "                return translated_audio + AudioSegment.silent(duration=compensation_ms)\n",
    "            else:\n",
    "                return translated_audio[:compensation_ms]\n",
    "        return translated_audio\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_phrase_timings(phrases: List[str], silences: List[float]) -> List[Dict]:\n",
    "        \"\"\"Calculate precise timings for each phrase\"\"\"\n",
    "        timings = []\n",
    "        current_time = 0.0\n",
    "        \n",
    "        for i, phrase in enumerate(phrases):\n",
    "            # Generate temporary audio to measure duration\n",
    "            with tempfile.NamedTemporaryFile() as tmp:\n",
    "                asyncio.run(edge_tts.Communicate(phrase).save(tmp.name))\n",
    "                audio = AudioSegment.from_file(tmp.name)\n",
    "                duration = len(audio) / 1000  # Convert ms to seconds\n",
    "            \n",
    "            timings.append({\n",
    "                \"start\": current_time,\n",
    "                \"end\": current_time + duration,\n",
    "                \"phrase\": phrase\n",
    "            })\n",
    "            \n",
    "            # Add silence after phrase if not last element\n",
    "            if i < len(silences):\n",
    "                current_time += duration + (silences[i] / 1000)\n",
    "        \n",
    "        return timings\n",
    "\n",
    "# ============== Main Processing Pipeline ==============\n",
    "class TranslationPipeline:\n",
    "    def __init__(self):\n",
    "        self.debug_log = []\n",
    "        \n",
    "    async def process_segment(self, segment: Dict, output_path: str):\n",
    "        \"\"\"Process single video segment with sync validation\"\"\"\n",
    "        # Generate translated audio\n",
    "        translated_audio = await self._generate_translated_audio(segment)\n",
    "\n",
    "        # Validate timing\n",
    "        original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "        validated_audio = SyncValidator.validate_segment_timing(\n",
    "            original_duration, translated_audio\n",
    "        )\n",
    "\n",
    "        # Save debug information\n",
    "        self._log_segment_debug(segment, translated_audio, validated_audio)\n",
    "\n",
    "        # Export final audio\n",
    "        validated_audio.export(output_path, format=\"wav\")\n",
    "        \n",
    "        async def _generate_translated_audio(self, segment: Dict) -> AudioSegment:\n",
    "            \"\"\"Generate translated audio with proper timing\"\"\"\n",
    "            combined_audio = AudioSegment.silent(segment[\"pre_silence\"])\n",
    "        \n",
    "        for i, phrase in enumerate(segment[\"phrases\"]):\n",
    "            # Generate phrase audio\n",
    "            phrase_audio = AudioProcessor.generate_phrase_audio(\n",
    "                phrase, segment[\"speed\"]\n",
    "            )\n",
    "            \n",
    "            # Add inter-phrase silence\n",
    "            if i > 0 and i <= len(segment[\"inter_silences\"]):\n",
    "                combined_audio += AudioSegment.silent(\n",
    "                    segment[\"inter_silences\"][i-1]\n",
    "                )\n",
    "            \n",
    "            combined_audio += phrase_audio\n",
    "        \n",
    "        # Add post-silence\n",
    "        combined_audio += AudioSegment.silent(segment[\"post_silence\"])\n",
    "        \n",
    "        return combined_audio\n",
    "    \n",
    "    def _log_segment_debug(self, segment, translated_audio, validated_audio):\n",
    "      \"\"\"Log segment debug information.\"\"\"\n",
    "      # Duration in seconds\n",
    "      original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "      translated_duration = len(translated_audio) / 1000\n",
    "      validated_duration = len(validated_audio) / 1000\n",
    "\n",
    "      # Log entry\n",
    "      log_entry = {\n",
    "          \"segment_start\": segment[\"start\"],\n",
    "          \"segment_end\": segment[\"end\"],\n",
    "          \"original_duration\": original_duration,\n",
    "          \"translated_duration\": translated_duration,\n",
    "          \"validated_duration\": validated_duration,\n",
    "          \"voice_speed\": segment[\"speed\"],\n",
    "          \"pre_silence\": segment[\"pre_silence\"],\n",
    "          \"post_silence\": segment[\"post_silence\"],\n",
    "          \"inter_silences\": segment[\"inter_silences\"],\n",
    "          \"phrases\": segment[\"phrases\"],\n",
    "      }\n",
    "      self.debug_log.append(log_entry)\n",
    "\n",
    "# ============== Helper Functions ==============\n",
    "def sanitize_silences(silences: List[float]) -> List[float]:\n",
    "    \"\"\"Ensure silences are within valid range\"\"\"\n",
    "    return [max(0, min(5000, s)) for s in silences]\n",
    "\n",
    "def parse_review_file(review_path: str) -> List[Dict]:\n",
    "    \"\"\"Parse review file with sanity checks\"\"\"\n",
    "    segments = []\n",
    "    current_segment = {}\n",
    "    segment_number = 0\n",
    "\n",
    "    with open(review_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Segment\"):\n",
    "          segment_number += 1\n",
    "          # Save previous segment if it exists\n",
    "          if current_segment:\n",
    "            segments.append(current_segment)\n",
    "          # Start a new segment\n",
    "          current_segment = {}\n",
    "          match = re.match(r\"Segment (\\d+) \\(start: (\\d+\\.?\\d*)s, end: (\\d+\\.?\\d*)s\\):\", line)\n",
    "          if match:\n",
    "            current_segment['segment_number'] = int(match.group(1))\n",
    "            current_segment['start'] = float(match.group(2))\n",
    "            current_segment['end'] = float(match.group(3))\n",
    "        elif line.startswith(\"**Original:**\"):\n",
    "          current_segment['original'] = line.split('**Original:**')[1].strip()\n",
    "        elif line.startswith(\"**Auto Translated:**\"):\n",
    "          current_segment['auto_translated'] = line.split('**Auto Translated:**')[1].strip()\n",
    "        elif line.startswith(\"**Final Translation:**\"):\n",
    "          current_segment['phrases'] = split_french_phrases(line.split('**Final Translation:**')[1].strip())\n",
    "        elif line.startswith(\"**Voice Speed:**\"):\n",
    "          current_segment['speed'] = line.split('**Voice Speed:**')[1].strip()\n",
    "        elif line.startswith(\"**Pre-Silence:**\"):\n",
    "          current_segment['pre_silence'] = int(line.split('**Pre-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Post-Silence:**\"):\n",
    "          current_segment['post_silence'] = int(line.split('**Post-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Inter-Phrase-Silence:**\"):\n",
    "          silences_str = line.split('**Inter-Phrase-Silence:**')[1].strip()\n",
    "          if silences_str:\n",
    "            current_segment['inter_silences'] = [int(s.strip()) for s in silences_str.split(',') if s.strip()]\n",
    "          else:\n",
    "            current_segment['inter_silences'] = []\n",
    "        elif line.startswith(\"**Décalage (local ms):**\"):\n",
    "          current_segment['decalage'] = int(line.split('**Décalage (local ms):**')[1].strip())\n",
    "      if current_segment:\n",
    "        segments.append(current_segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def split_french_phrases(text: str) -> List[str]:\n",
    "    \"\"\"Splits a French text into phrases using common punctuation marks.\"\"\"\n",
    "    # Split by periods, question marks, and exclamation points, but keep the delimiters.\n",
    "    phrases = re.split(r\"([.?!])\", text)\n",
    "    # Recombine the delimiters with the preceding text.\n",
    "    # Here's the fix: convert phrases to an iterator explicitly\n",
    "    phrases_iter = iter(phrases)\n",
    "    phrases = [phrase + next(phrases_iter, '') for phrase in phrases]\n",
    "    # Clean up: remove empty strings and strip whitespace.\n",
    "    phrases = [p.strip() for p in phrases if p.strip()]\n",
    "    return phrases\n",
    "\n",
    "\n",
    "# ============== Main Execution ==============\n",
    "async def main():\n",
    "    # Initialize pipeline\n",
    "    pipeline = TranslationPipeline()\n",
    "\n",
    "    # Load review configuration\n",
    "    review_file_path = \"translation_review.txt\"\n",
    "    segments = parse_review_file(review_file_path)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = OUTPUT_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each segment\n",
    "    tasks = []\n",
    "    audio_segments = []\n",
    "    for idx, segment in enumerate(segments):\n",
    "        output_path = os.path.join(output_dir, f\"segment_{idx+1}.wav\")\n",
    "        task = pipeline.process_segment(segment, output_path)\n",
    "        tasks.append(task)\n",
    "        audio_segments.append(output_path)\n",
    "\n",
    "    # Run all tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Final video assembly\n",
    "    input_video_path = INPUT_VIDEO\n",
    "    original_video = VideoFileClip(input_video_path)\n",
    "    \n",
    "    # Load the audio segments\n",
    "    audio_clips = [AudioFileClip(audio_path) for audio_path in audio_segments]\n",
    "    \n",
    "    # Prepare video segments\n",
    "    video_segments = []\n",
    "    current_time = 0\n",
    "    for i, segment in enumerate(segments):\n",
    "        start_time = segment[\"start\"]\n",
    "        end_time = segment[\"end\"]\n",
    "        \n",
    "        # Extract the video clip for the segment\n",
    "        video_clip = original_video.subclip(start_time, end_time)\n",
    "        \n",
    "        # Set the audio of the video clip\n",
    "        video_clip = video_clip.set_audio(audio_clips[i])\n",
    "        \n",
    "        video_segments.append(video_clip)\n",
    "        current_time = end_time\n",
    "\n",
    "    # Concatenate all video segments\n",
    "    final_video = concatenate_videoclips(video_segments)\n",
    "    \n",
    "    # Set the FPS to the original video's FPS\n",
    "    final_video = final_video.set_fps(original_video.fps)\n",
    "\n",
    "    # Write the final video file\n",
    "    output_video_path = os.path.join(output_dir, \"final_translated_video.mp4\")\n",
    "    final_video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\", temp_audiofile='temp-audio.m4a', remove_temp=True)\n",
    "    \n",
    "    print(f\"✅ Final translated video created at: {output_video_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291994da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: It seems you are trying to call asyncio.run from within an already running event loop.\n",
      "This can happen in interactive environments like Jupyter notebooks.\n",
      "Please try running this script from a regular Python environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ffmpeg\n",
    "import pydub\n",
    "import pysrt\n",
    "import time\n",
    "import asyncio\n",
    "import edge_tts\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
    "from faster_whisper import WhisperModel\n",
    "from shutil import which\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import aiohttp\n",
    "import ssl\n",
    "import random\n",
    "from pydub.silence import detect_nonsilent\n",
    "from typing import List, Dict\n",
    "\n",
    "# ============== Configuration ==============\n",
    "FFMPEG_PATH = which(\"ffmpeg\")\n",
    "INPUT_VIDEO = \"to translate/4.2.4_Configuration de la solution_Avr_10_Latest.mp4\"\n",
    "BASE_NAME = os.path.splitext(os.path.basename(INPUT_VIDEO))[0]\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = f\"{BASE_NAME}_run_{TIMESTAMP}\"\n",
    "MODEL_SIZE = \"small\"\n",
    "USE_EDGE_TTS = True\n",
    "\n",
    "# ============== Audio Processing Functions ==============\n",
    "class AudioProcessor:\n",
    "    @staticmethod\n",
    "    def apply_speed_adjustment(raw_audio: AudioSegment, speed_setting: str) -> AudioSegment:\n",
    "        \"\"\"Apply speed adjustment with duration compensation\"\"\"\n",
    "        speed_factor = 1 + (int(speed_setting.strip('%')) / 100)\n",
    "        original_duration = len(raw_audio)\n",
    "        \n",
    "        # Speed adjustment with crossfade to avoid clicks\n",
    "        sped_up = raw_audio.speedup(\n",
    "            playback_speed=speed_factor,\n",
    "            chunk_size=150,\n",
    "            crossfade=25\n",
    "        )\n",
    "        \n",
    "        # Calculate compensation\n",
    "        new_duration = len(sped_up)\n",
    "        compensation_ms = original_duration - new_duration\n",
    "        \n",
    "        if compensation_ms > 0:\n",
    "            return sped_up + AudioSegment.silent(duration=compensation_ms)\n",
    "        return sped_up\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_phrase_audio(text: str, voice_speed: str) -> AudioSegment:\n",
    "        \"\"\"Generate phrase audio with natural ending detection\"\"\"\n",
    "        async def _generate():\n",
    "            communicate = edge_tts.Communicate(text)\n",
    "            return await communicate\n",
    "        \n",
    "        raw_audio = asyncio.run(_generate()).audio\n",
    "        processed = AudioProcessor.apply_speed_adjustment(raw_audio, voice_speed)\n",
    "        \n",
    "        # Detect natural speech endings\n",
    "        non_silent = detect_nonsilent(\n",
    "            processed, \n",
    "            min_silence_len=50,\n",
    "            silence_thresh=processed.dBFS - 16\n",
    "        )\n",
    "        \n",
    "        if non_silent:\n",
    "            end_pad = 150  # Minimum ending padding\n",
    "            new_end = max(non_silent[-1][1] + end_pad, len(processed))\n",
    "            return processed[:new_end]\n",
    "        return processed\n",
    "\n",
    "# ============== Timing Synchronization ==============\n",
    "class SyncValidator:\n",
    "    @staticmethod\n",
    "    def validate_segment_timing(original_duration: float, translated_audio: AudioSegment) -> AudioSegment:\n",
    "        \"\"\"Ensure audio duration matches video segment duration\"\"\"\n",
    "        audio_duration = len(translated_audio) / 1000  # Convert ms to seconds\n",
    "        drift = original_duration - audio_duration\n",
    "        \n",
    "        if abs(drift) > 0.5:  # 500ms tolerance\n",
    "            compensation_ms = int(drift * 1000)\n",
    "            if compensation_ms > 0:\n",
    "                return translated_audio + AudioSegment.silent(duration=compensation_ms)\n",
    "            else:\n",
    "                return translated_audio[:compensation_ms]\n",
    "        return translated_audio\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_phrase_timings(phrases: List[str], silences: List[float]) -> List[Dict]:\n",
    "        \"\"\"Calculate precise timings for each phrase\"\"\"\n",
    "        timings = []\n",
    "        current_time = 0.0\n",
    "        \n",
    "        for i, phrase in enumerate(phrases):\n",
    "            # Generate temporary audio to measure duration\n",
    "            with tempfile.NamedTemporaryFile() as tmp:\n",
    "                asyncio.run(edge_tts.Communicate(phrase).save(tmp.name))\n",
    "                audio = AudioSegment.from_file(tmp.name)\n",
    "                duration = len(audio) / 1000  # Convert ms to seconds\n",
    "            \n",
    "            timings.append({\n",
    "                \"start\": current_time,\n",
    "                \"end\": current_time + duration,\n",
    "                \"phrase\": phrase\n",
    "            })\n",
    "            \n",
    "            # Add silence after phrase if not last element\n",
    "            if i < len(silences):\n",
    "                current_time += duration + (silences[i] / 1000)\n",
    "        \n",
    "        return timings\n",
    "\n",
    "# ============== Main Processing Pipeline ==============\n",
    "class TranslationPipeline:\n",
    "    def __init__(self):\n",
    "        self.debug_log = []\n",
    "        \n",
    "    async def process_segment(self, segment: Dict, output_path: str):\n",
    "        \"\"\"Process single video segment with sync validation\"\"\"\n",
    "        # Generate translated audio\n",
    "        translated_audio = await self._generate_translated_audio(segment)\n",
    "        \n",
    "        # Validate timing\n",
    "        original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "        validated_audio = SyncValidator.validate_segment_timing(\n",
    "            original_duration, translated_audio\n",
    "        )\n",
    "        \n",
    "        # Save debug information\n",
    "        self._log_segment_debug(segment, translated_audio, validated_audio)\n",
    "        \n",
    "        # Export final audio\n",
    "        validated_audio.export(output_path, format=\"wav\")\n",
    "    \n",
    "    async def _generate_translated_audio(self, segment: Dict) -> AudioSegment:\n",
    "        \"\"\"Generate translated audio with proper timing\"\"\"\n",
    "        combined_audio = AudioSegment.silent(segment[\"pre_silence\"])\n",
    "        \n",
    "        for i, phrase in enumerate(segment[\"phrases\"]):\n",
    "            # Generate phrase audio\n",
    "            phrase_audio = AudioProcessor.generate_phrase_audio(\n",
    "                phrase, segment[\"speed\"]\n",
    "            )\n",
    "            \n",
    "            # Add inter-phrase silence\n",
    "            if i > 0 and i <= len(segment[\"inter_silences\"]):\n",
    "                combined_audio += AudioSegment.silent(\n",
    "                    segment[\"inter_silences\"][i-1]\n",
    "                )\n",
    "            \n",
    "            combined_audio += phrase_audio\n",
    "        \n",
    "        # Add post-silence\n",
    "        combined_audio += AudioSegment.silent(segment[\"post_silence\"])\n",
    "        \n",
    "        return combined_audio\n",
    "    \n",
    "    def _log_segment_debug(self, segment, translated_audio, validated_audio):\n",
    "      \"\"\"Log segment debug information.\"\"\"\n",
    "      # Duration in seconds\n",
    "      original_duration = segment[\"end\"] - segment[\"start\"]\n",
    "      translated_duration = len(translated_audio) / 1000\n",
    "      validated_duration = len(validated_audio) / 1000\n",
    "\n",
    "      # Log entry\n",
    "      log_entry = {\n",
    "          \"segment_start\": segment[\"start\"],\n",
    "          \"segment_end\": segment[\"end\"],\n",
    "          \"original_duration\": original_duration,\n",
    "          \"translated_duration\": translated_duration,\n",
    "          \"validated_duration\": validated_duration,\n",
    "          \"voice_speed\": segment[\"speed\"],\n",
    "          \"pre_silence\": segment[\"pre_silence\"],\n",
    "          \"post_silence\": segment[\"post_silence\"],\n",
    "          \"inter_silences\": segment[\"inter_silences\"],\n",
    "          \"phrases\": segment[\"phrases\"],\n",
    "      }\n",
    "      self.debug_log.append(log_entry)\n",
    "\n",
    "# ============== Helper Functions ==============\n",
    "def sanitize_silences(silences: List[float]) -> List[float]:\n",
    "    \"\"\"Ensure silences are within valid range\"\"\"\n",
    "    return [max(0, min(5000, s)) for s in silences]\n",
    "\n",
    "def parse_review_file(review_path: str) -> List[Dict]:\n",
    "    \"\"\"Parse review file with sanity checks\"\"\"\n",
    "    segments = []\n",
    "    current_segment = {}\n",
    "    segment_number = 0\n",
    "\n",
    "    with open(review_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Segment\"):\n",
    "          segment_number += 1\n",
    "          # Save previous segment if it exists\n",
    "          if current_segment:\n",
    "            segments.append(current_segment)\n",
    "          # Start a new segment\n",
    "          current_segment = {}\n",
    "          match = re.match(r\"Segment (\\d+) \\(start: (\\d+\\.?\\d*)s, end: (\\d+\\.?\\d*)s\\):\", line)\n",
    "          if match:\n",
    "            current_segment['segment_number'] = int(match.group(1))\n",
    "            current_segment['start'] = float(match.group(2))\n",
    "            current_segment['end'] = float(match.group(3))\n",
    "        elif line.startswith(\"**Original:**\"):\n",
    "          current_segment['original'] = line.split('**Original:**')[1].strip()\n",
    "        elif line.startswith(\"**Auto Translated:**\"):\n",
    "          current_segment['auto_translated'] = line.split('**Auto Translated:**')[1].strip()\n",
    "        elif line.startswith(\"**Final Translation:**\"):\n",
    "          current_segment['phrases'] = split_french_phrases(line.split('**Final Translation:**')[1].strip())\n",
    "        elif line.startswith(\"**Voice Speed:**\"):\n",
    "          current_segment['speed'] = line.split('**Voice Speed:**')[1].strip()\n",
    "        elif line.startswith(\"**Pre-Silence:**\"):\n",
    "          current_segment['pre_silence'] = int(line.split('**Pre-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Post-Silence:**\"):\n",
    "          current_segment['post_silence'] = int(line.split('**Post-Silence:**')[1].strip())\n",
    "        elif line.startswith(\"**Inter-Phrase-Silence:**\"):\n",
    "          silences_str = line.split('**Inter-Phrase-Silence:**')[1].strip()\n",
    "          if silences_str:\n",
    "            current_segment['inter_silences'] = [int(s.strip()) for s in silences_str.split(',') if s.strip()]\n",
    "          else:\n",
    "            current_segment['inter_silences'] = []\n",
    "        elif line.startswith(\"**Décalage (local ms):**\"):\n",
    "          current_segment['decalage'] = int(line.split('**Décalage (local ms):**')[1].strip())\n",
    "      if current_segment:\n",
    "        segments.append(current_segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def split_french_phrases(text: str) -> List[str]:\n",
    "    \"\"\"Splits a French text into phrases using common punctuation marks.\"\"\"\n",
    "    # Split by periods, question marks, and exclamation points, but keep the delimiters.\n",
    "    phrases = re.split(r\"([.?!])\", text)\n",
    "    # Recombine the delimiters with the preceding text.\n",
    "    # Here's the fix: convert phrases to an iterator explicitly\n",
    "    phrases_iter = iter(phrases)\n",
    "    phrases = [phrase + next(phrases_iter, '') for phrase in phrases]\n",
    "    # Clean up: remove empty strings and strip whitespace.\n",
    "    phrases = [p.strip() for p in phrases if p.strip()]\n",
    "    return phrases\n",
    "\n",
    "# ============== Main Execution ==============\n",
    "async def main():\n",
    "    # Initialize pipeline\n",
    "    pipeline = TranslationPipeline()\n",
    "\n",
    "    # Load review configuration\n",
    "    review_file_path = \"translation_review.txt\"\n",
    "    segments = parse_review_file(review_file_path)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = OUTPUT_DIR\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each segment\n",
    "    tasks = []\n",
    "    audio_segments = []\n",
    "    for idx, segment in enumerate(segments):\n",
    "        output_path = os.path.join(output_dir, f\"segment_{idx+1}.wav\")\n",
    "        task = pipeline.process_segment(segment, output_path)\n",
    "        tasks.append(task)\n",
    "        audio_segments.append(output_path)\n",
    "\n",
    "    # Run all tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Final video assembly\n",
    "    input_video_path = INPUT_VIDEO\n",
    "    original_video = VideoFileClip(input_video_path)\n",
    "    \n",
    "    # Load the audio segments\n",
    "    audio_clips = [AudioFileClip(audio_path) for audio_path in audio_segments]\n",
    "    \n",
    "    # Prepare video segments\n",
    "    video_segments = []\n",
    "    current_time = 0\n",
    "    for i, segment in enumerate(segments):\n",
    "        start_time = segment[\"start\"]\n",
    "        end_time = segment[\"end\"]\n",
    "        \n",
    "        # Extract the video clip for the segment\n",
    "        video_clip = original_video.subclip(start_time, end_time)\n",
    "        \n",
    "        # Set the audio of the video clip\n",
    "        video_clip = video_clip.set_audio(audio_clips[i])\n",
    "        \n",
    "        video_segments.append(video_clip)\n",
    "        current_time = end_time\n",
    "\n",
    "    # Concatenate all video segments\n",
    "    final_video = concatenate_videoclips(video_segments)\n",
    "    \n",
    "    # Set the FPS to the original video's FPS\n",
    "    final_video = final_video.set_fps(original_video.fps)\n",
    "\n",
    "    # Write the final video file\n",
    "    output_video_path = os.path.join(output_dir, \"final_translated_video.mp4\")\n",
    "    final_video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\", temp_audiofile='temp-audio.m4a', remove_temp=True)\n",
    "    \n",
    "    print(f\"✅ Final translated video created at: {output_video_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except RuntimeError as e:\n",
    "        if \"cannot be called from a running event loop\" in str(e):\n",
    "            print(\"Error: It seems you are trying to call asyncio.run from within an already running event loop.\")\n",
    "            print(\"This can happen in interactive environments like Jupyter notebooks.\")\n",
    "            print(\"Please try running this script from a regular Python environment.\")\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af07bad",
   "metadata": {},
   "source": [
    "23APR25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307134f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] TTS attempt 1 failed for phrase: Nous allons voir les configurations de l'application EPM.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE96BA0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Nous allons voir les configurations de l'application EPM.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE969F0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous verrons comment la sécurité fonctionne dans l'application EPM et nous couvrirons comment créer et configurer des formulaires de données.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE96A80> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: la sécurité par tâches ou le flux de travail, la sécurité des règles métier et la sécurité des données.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE96180> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: la sécurité par tâches ou le flux de travail, la sécurité des règles métier et la sécurité des données.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE976E0> [Une connexion existante a dû être fermée par l’hôte distant]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 413\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# Nous pouvons maintenant utiliser run_until_complete même si un loop tourne déjà\u001b[39;00m\n\u001b[32m    412\u001b[39m     loop = asyncio.get_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AI PROJECTS\\video_transcript\\myenv\\Lib\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AI PROJECTS\\video_transcript\\myenv\\Lib\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = select.select(r, w, w, timeout)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] TTS attempt 1 failed for phrase: Ici, le matériel OFS sera divisé par le volume OFS.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702210> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais annuler dans ce cas.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702B10> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Pour voir une règle métier, cliquer sur l'icône Navigateur et cliquer sur Règles.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA15ED370> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Pour voir une règle métier, cliquer sur l'icône Navigateur et cliquer sur Règles.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701880> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je peux importer et je peux voir les propriétés.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702570> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Comme j'ai sélectionné le cube et l'application, toutes ces informations sont prépopulées.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17027B0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Le système indiquera que la règle a été créé et qu'on peut y apporter des modifications.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17029F0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais sélectionner la vue de script.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701D00> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous avons toutes les dimensions requises dont nous avons besoin ici.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701AC0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Nous avons toutes les dimensions requises dont nous avons besoin ici.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701B50> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: La règle va copier les données de FY24 pour le scénario plan, working, de la contrepartie de l'allocation d'entité, au plan FY23, working, contrepartie de l'allocation d'entité.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702CC0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Pour copier des données, nous devons simplement exécuter la règle métier et il copiera les données de FY24 à FY23.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701F40> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Pour copier des données, nous devons simplement exécuter la règle métier et il copiera les données de FY24 à FY23.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702210> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 3 failed for phrase: Pour copier des données, nous devons simplement exécuter la règle métier et il copiera les données de FY24 à FY23.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702840> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Error] All TTS attempts failed; inserted 500ms silence at C:\\Users\\061181~1\\AppData\\Local\\Temp\\tmp_7_0.mp3\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous pouvons également céduler la règle métier dans nos automations et gérer dans le cadre des travaux quotidiens d'automatisation.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17022A0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Regardons la sécurité dimensionnelle.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA15ED370> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais aller dans le dossier Démo et j'ai un formulaire et en cliquant sur ceci, je vais ajouter un groupe.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701C70> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Voyons comment créer et configurer un formulaire de données. À partir de la page d'accueil cliquez sur la carte Données.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17029F0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous devons entrer le nom.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702960> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Cliquer sur l'icône me donnera le sélecteur de membres et je peux ajouter le membre requis.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702690> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 2 failed for phrase: Cliquer sur l'icône me donnera le sélecteur de membres et je peux ajouter le membre requis.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17020F0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: C'est là que nous pouvons ajouter une règle métier que nous avons créée pour cette application.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701C70> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous cliquerons sur OK.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702180> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais cliquer sur OK.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE97770> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je vais les enregistrer.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17017F0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Je veux également refléter ces données pour tous les autres chiffres.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x000002101DE97770> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Est-ce un ajustement proportionnel ou un ajustement uniforme?\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA17023C0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous pouvons définir la langue et les zones.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1702B10> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "[Warning] TTS attempt 1 failed for phrase: Nous définirons les variables utilisateur à partir d'ici en sélectionnant le sélecteur de membre et en sélectionnant le membre respectif et en cliquant sur OK, puis en l'enregistrant.\n",
      "Cannot connect to host speech.platform.bing.com:443 ssl:<ssl.SSLContext object at 0x0000020FA1701BE0> [Une connexion existante a dû être fermée par l’hôte distant]\n",
      "Moviepy - Building video 4.2.4_Configuration de la solution_Avr_10_Latest_run_20250423_112245\\4.2.4_Configuration de la solution_Avr_10_Latest-french.mp4.\n",
      "MoviePy - Writing audio in temp-audio.m4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video 4.2.4_Configuration de la solution_Avr_10_Latest_run_20250423_112245\\4.2.4_Configuration de la solution_Avr_10_Latest-french.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready 4.2.4_Configuration de la solution_Avr_10_Latest_run_20250423_112245\\4.2.4_Configuration de la solution_Avr_10_Latest-french.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ffmpeg\n",
    "import pysrt\n",
    "import tempfile\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "from faster_whisper import WhisperModel\n",
    "from shutil import which\n",
    "import edge_tts\n",
    "import aiohttp\n",
    "import random\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ── Configuration ────────────────────────────────────────────────────────────────\n",
    "ffmpeg_path = which(\"ffmpeg\")\n",
    "if not ffmpeg_path:\n",
    "    raise RuntimeError(\"ffmpeg not found. Please install ffmpeg first.\")\n",
    "\n",
    "input_video     = \"to translate/4.2.4_Configuration de la solution_Avr_10_Latest.mp4\"\n",
    "base_name       = os.path.splitext(os.path.basename(input_video))[0]\n",
    "timestamp       = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir      = f\"{base_name}_run_{timestamp}\"\n",
    "model_size      = \"small\"\n",
    "USE_EDGE_TTS    = True\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "extracted_audio = os.path.join(output_dir, f\"{base_name}-extracted-audio.wav\")\n",
    "subtitle_file   = os.path.join(output_dir, f\"{base_name}-english.srt\")\n",
    "review_file     = os.path.join(output_dir, \"translation_review.txt\")\n",
    "debug_log_file  = os.path.join(output_dir, \"translation_debug_log.txt\")\n",
    "translated_audio= os.path.join(output_dir, f\"{base_name}-french.wav\")\n",
    "output_video    = os.path.join(output_dir, f\"{base_name}-french.mp4\")\n",
    "\n",
    "# ── Helpers: audio extraction & whisper transcription ────────────────────────────\n",
    "def extract_audio():\n",
    "    ffmpeg.input(input_video).output(extracted_audio, ac=1, ar=16000) \\\n",
    "        .overwrite_output().run(capture_stdout=True, capture_stderr=True)\n",
    "    return extracted_audio\n",
    "\n",
    "def transcribe(path):\n",
    "    model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "    segments, info = model.transcribe(path, beam_size=5)\n",
    "    return info.language, [\n",
    "        {\"start\": seg.start, \"end\": seg.end, \"text\": seg.text.strip()}\n",
    "        for seg in segments\n",
    "    ]\n",
    "\n",
    "def time_to_subrip(seconds: float) -> pysrt.SubRipTime:\n",
    "    h = int(seconds // 3600)\n",
    "    seconds %= 3600\n",
    "    m = int(seconds // 60)\n",
    "    seconds %= 60\n",
    "    ms = int((seconds - int(seconds)) * 1000)\n",
    "    return pysrt.SubRipTime(hours=h, minutes=m, seconds=int(seconds), milliseconds=ms)\n",
    "\n",
    "def generate_subtitle_file(segments, outpath):\n",
    "    subs = pysrt.SubRipFile()\n",
    "    for i, seg in enumerate(segments, 1):\n",
    "        subs.append(pysrt.SubRipItem(\n",
    "            index=i,\n",
    "            start=time_to_subrip(seg[\"start\"]),\n",
    "            end=time_to_subrip(seg[\"end\"]),\n",
    "            text=seg[\"text\"]\n",
    "        ))\n",
    "    subs.save(outpath, encoding=\"utf-8\")\n",
    "    return outpath\n",
    "\n",
    "# ── Text grouping & review file generation ─────────────────────────────────────\n",
    "def split_long_groups(groups, max_dur):\n",
    "    new = []\n",
    "    for grp in groups:\n",
    "        start = grp[0].start.ordinal/1000\n",
    "        end   = grp[-1].end.ordinal/1000\n",
    "        if end - start <= max_dur:\n",
    "            new.append(grp)\n",
    "            continue\n",
    "        temp, ts = [], start\n",
    "        last_safe = None\n",
    "        for idx, item in enumerate(grp):\n",
    "            temp.append(item)\n",
    "            if re.search(r\"[.,!?]$\", item.text.strip()):\n",
    "                last_safe = idx\n",
    "            cur_end = item.end.ordinal/1000\n",
    "            if cur_end - ts >= max_dur:\n",
    "                if last_safe is not None:\n",
    "                    new.append(temp[:last_safe+1])\n",
    "                    temp = temp[last_safe+1:]\n",
    "                    ts   = temp[0].start.ordinal/1000 if temp else cur_end\n",
    "                else:\n",
    "                    new.append(temp)\n",
    "                    temp = []\n",
    "                    ts   = cur_end\n",
    "                last_safe = None\n",
    "        if temp:\n",
    "            new.append(temp)\n",
    "    return new\n",
    "\n",
    "def enforce_punctuation_boundaries(groups):\n",
    "    i = 0\n",
    "    safe = r\"[.!?,;:]$\"\n",
    "    while i < len(groups):\n",
    "        last = groups[i][-1].text.strip()\n",
    "        if not re.search(safe, last):\n",
    "            if i+1 < len(groups):\n",
    "                groups[i] += groups.pop(i+1)\n",
    "            else:\n",
    "                groups[i][-1].text += \".\"\n",
    "        else:\n",
    "            i += 1\n",
    "    return groups\n",
    "\n",
    "def generate_translation_review_file(source_path, review_path,\n",
    "                                     from_lang=\"en\", to_lang=\"fr\",\n",
    "                                     max_group_duration_secs=25.0):\n",
    "    translator = GoogleTranslator(source=from_lang, target=to_lang)\n",
    "    subs       = pysrt.open(source_path)\n",
    "    sentence_end = re.compile(r\"[.!?]\\s*$\")\n",
    "\n",
    "    # 1) group subs by sentence\n",
    "    groups, cur = [], []\n",
    "    for sub in subs:\n",
    "        cur.append(sub)\n",
    "        if sentence_end.search(sub.text):\n",
    "            groups.append(cur)\n",
    "            cur = []\n",
    "    if cur: groups.append(cur)\n",
    "\n",
    "    # 2) split long groups, enforce punctuation\n",
    "    groups = split_long_groups(groups, max_group_duration_secs)\n",
    "    groups = enforce_punctuation_boundaries(groups)\n",
    "\n",
    "    # 3) write review file\n",
    "    with open(review_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Translation Review File\\n\")\n",
    "        f.write(\"Edit **Final Translation**, **Voice Speed**, **Pre‑Silence**, **Post‑Silence**, **Inter‑Phrase‑Silence**\\n\")\n",
    "        f.write(\"----------------------------------------------------------------------------- \\n\\n\")\n",
    "        for idx, grp in enumerate(groups, 1):\n",
    "            start = grp[0].start.ordinal/1000\n",
    "            end   = grp[-1].end.ordinal/1000\n",
    "            orig  = \" \".join(s.text for s in grp)\n",
    "            auto  = translator.translate(text=orig)\n",
    "            # default values\n",
    "            phrases = split_french_phrases(auto)\n",
    "            inter_default = \",\".join(\"0\" for _ in range(len(phrases)-1))\n",
    "            f.write(f\"Segment {idx} (start: {start:.2f}s, end: {end:.2f}s):\\n\")\n",
    "            f.write(f\"**Original:** {orig}\\n\")\n",
    "            f.write(f\"**Auto Translated:** {auto}\\n\")\n",
    "            f.write(f\"**Final Translation:** {auto}\\n\")\n",
    "            f.write(f\"**Voice Speed:** +0%\\n\")\n",
    "            f.write(f\"**Pre-Silence:** 100\\n\")\n",
    "            f.write(f\"**Post-Silence:** 100\\n\")\n",
    "            f.write(f\"**Inter-Phrase-Silence:** {inter_default}\\n\")\n",
    "            f.write(\"-----------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "    input(\"Review file ready. Press Enter to continue…\")\n",
    "    return groups\n",
    "\n",
    "# ── Review overrides parsing ────────────────────────────────────────────────────\n",
    "def parse_review_overrides(review_file_path):\n",
    "    text   = open(review_file_path, \"r\", encoding=\"utf-8\").read()\n",
    "    blocks = re.split(r\"(?m)^-{3,}\\s*$\", text)\n",
    "    overrides = []\n",
    "    for idx, blk in enumerate(blocks, start=1):\n",
    "        blk = blk.strip()\n",
    "        if not blk or blk.startswith(\"Translation Review File\"):\n",
    "            continue\n",
    "        # defaults\n",
    "        ft       = None\n",
    "        vs       = \"+0%\"\n",
    "        pre_ms   = 0.0\n",
    "        post_ms  = 100.0\n",
    "        inter_ms = []\n",
    "        for line in blk.splitlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"**Final Translation:**\"):\n",
    "                ft = line.split(\"**Final Translation:**\",1)[1].strip()\n",
    "            elif line.startswith(\"**Voice Speed:**\"):\n",
    "                vs = line.split(\"**Voice Speed:**\",1)[1].strip()\n",
    "            elif line.startswith(\"**Pre-Silence:**\"):\n",
    "                try: pre_ms = float(line.split(\"**Pre-Silence:**\",1)[1].strip())\n",
    "                except: pass\n",
    "            elif line.startswith(\"**Post-Silence:**\"):\n",
    "                try: post_ms = float(line.split(\"**Post-Silence:**\",1)[1].strip())\n",
    "                except: pass\n",
    "            elif line.startswith(\"**Inter-Phrase-Silence:**\"):\n",
    "                parts = line.split(\"**Inter-Phrase-Silence:**\",1)[1].strip()\n",
    "                if parts:\n",
    "                    try:\n",
    "                        inter_ms = [float(x) for x in parts.split(\",\")]\n",
    "                    except: pass\n",
    "        overrides.append({\n",
    "            \"final_translation\":     ft,\n",
    "            \"voice_speed\":           vs,\n",
    "            \"pre_silence\":           pre_ms,\n",
    "            \"post_silence\":          post_ms,\n",
    "            \"inter_phrase_silences\": inter_ms\n",
    "        })\n",
    "    return overrides\n",
    "\n",
    "# ── Phrase splitting & weighting ───────────────────────────────────────────────\n",
    "def split_french_phrases(text):\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+(?=[A-Z])\", text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def calculate_phrase_weights(orig, phrases):\n",
    "    counts = [len(p.split()) for p in phrases]\n",
    "    total  = sum(counts)\n",
    "    if total == 0:\n",
    "        return [1/len(phrases)]*len(phrases)\n",
    "    return [c/total for c in counts]\n",
    "\n",
    "# ── Audio adjustments & TTS ─────────────────────────────────────────────────────\n",
    "def adjust_audio_duration(audio, target_dur):\n",
    "    cur = audio.duration_seconds\n",
    "    diff= target_dur - cur\n",
    "    if diff > 0.1:\n",
    "        return audio + AudioSegment.silent(duration=diff*1000)\n",
    "    return audio\n",
    "\n",
    "def change_playback_speed(sound, speed=1.0):\n",
    "    new_rate = int(sound.frame_rate * speed)\n",
    "    altered  = sound._spawn(sound.raw_data, overrides={\"frame_rate\": new_rate})\n",
    "    return altered.set_frame_rate(sound.frame_rate)\n",
    "\n",
    "async def robust_synthesize_phrase(\n",
    "    phrase: str,\n",
    "    outpath: str,\n",
    "    voice: str = \"fr-FR-DeniseNeural\",\n",
    "    rate: str = \"+0%\",\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession(\n",
    "                timeout=aiohttp.ClientTimeout(total=30)\n",
    "            ) as session:\n",
    "                communicate = edge_tts.Communicate(\n",
    "                    text=phrase,\n",
    "                    voice=voice,\n",
    "                    rate=rate\n",
    "                )\n",
    "                await communicate.save(outpath)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] TTS attempt {attempt+1} failed for phrase: {phrase}\\n{e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    # Si après tous les essais ça échoue, on écrit un silence de secours\n",
    "    silent = AudioSegment.silent(duration=500)  # 0.5s de silence\n",
    "    silent.export(outpath, format=\"mp3\")\n",
    "    print(f\"[Error] All TTS attempts failed; inserted 500ms silence at {outpath}\")\n",
    "\n",
    "\n",
    "\n",
    "async def synthesize_phrase_edge_hybrid(phrase, outpath, voice=\"fr-FR-DeniseNeural\", rate=\"+0%\"):\n",
    "    await robust_synthesize_phrase(phrase, outpath, voice, rate)\n",
    "\n",
    "# ── Main async with updated generation ──────────────────────────────────────────\n",
    "async def async_generate_translated_audio_with_sync_using_review(\n",
    "    subtitle_source_path, output_audio_path,\n",
    "    debug_log_path, review_file_path\n",
    "):\n",
    "    groups    = generate_translation_review_file(subtitle_source_path, review_file_path)\n",
    "    overrides = parse_review_overrides(review_file_path)\n",
    "\n",
    "    default_ov = {\n",
    "        \"final_translation\":     None,\n",
    "        \"voice_speed\":           \"+0%\",\n",
    "        \"pre_silence\":           0.0,\n",
    "        \"post_silence\":          100.0,\n",
    "        \"inter_phrase_silences\": []\n",
    "    }\n",
    "    while len(overrides) < len(groups):\n",
    "        overrides.append(default_ov.copy())\n",
    "\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "    debug    = []\n",
    "\n",
    "    for idx, group in enumerate(groups):\n",
    "        start_s, end_s = group[0].start.ordinal/1000, group[-1].end.ordinal/1000\n",
    "        seg_dur  = end_s - start_s\n",
    "        total_ms = int(seg_dur * 1000)\n",
    "\n",
    "        ovr     = overrides[idx]\n",
    "        text    = ovr[\"final_translation\"] or \" \".join(s.text for s in group)\n",
    "        rate    = ovr[\"voice_speed\"]\n",
    "        pre_ms  = ovr[\"pre_silence\"]\n",
    "        post_ms = ovr[\"post_silence\"]\n",
    "        inter   = ovr[\"inter_phrase_silences\"]\n",
    "\n",
    "        phrases    = split_french_phrases(text)\n",
    "        weights    = calculate_phrase_weights(text, phrases)\n",
    "        content_ms = max(0, total_ms - pre_ms - post_ms)\n",
    "\n",
    "        # synth phrases\n",
    "        phrase_audios = []\n",
    "        for i, ph in enumerate(phrases):\n",
    "            dur = content_ms * weights[i] / 1000.0\n",
    "            tmp = os.path.join(tempfile.gettempdir(), f\"tmp_{idx}_{i}.mp3\")\n",
    "            await synthesize_phrase_edge_hybrid(ph, tmp, voice=\"fr-FR-DeniseNeural\", rate=rate)\n",
    "            aud = AudioSegment.from_mp3(tmp)\n",
    "            os.remove(tmp)\n",
    "            aud = adjust_audio_duration(aud, dur)\n",
    "            phrase_audios.append(aud)\n",
    "\n",
    "        # TTS over‑run protection\n",
    "        sum_tts = sum(a.duration_seconds*1000 for a in phrase_audios)\n",
    "        if sum_tts > content_ms and sum_tts > 0:\n",
    "            factor_audio = content_ms / sum_tts\n",
    "            phrase_audios = [change_playback_speed(aud, factor_audio) for aud in phrase_audios]\n",
    "            sum_tts = sum(a.duration_seconds*1000 for a in phrase_audios)\n",
    "\n",
    "        # inter‑phrase auto‑fit (10% margin)\n",
    "        available   = total_ms - pre_ms - post_ms - sum_tts\n",
    "        total_inter = sum(inter)\n",
    "        margin      = 0.9\n",
    "        if total_inter > 0 and total_inter > available * margin:\n",
    "            factor = (available * margin) / total_inter\n",
    "            inter  = [int(ms * factor) for ms in inter]\n",
    "\n",
    "        # rebuild with silences\n",
    "        seq = []\n",
    "        for i, aud in enumerate(phrase_audios):\n",
    "            seq.append(aud)\n",
    "            if i < len(inter):\n",
    "                seq.append(AudioSegment.silent(duration=inter[i]))\n",
    "\n",
    "        # assemble segment\n",
    "        seg = AudioSegment.silent(duration=pre_ms)\n",
    "        for clip in seq:\n",
    "            seg += clip\n",
    "        seg += AudioSegment.silent(duration=post_ms)\n",
    "\n",
    "        # strip leading TTS silence\n",
    "        non = detect_nonsilent(seg, min_silence_len=1, silence_thresh=seg.dBFS-16)\n",
    "        if non:\n",
    "            seg = seg[non[0][0]:]\n",
    "        seg = AudioSegment.silent(duration=pre_ms) + seg\n",
    "\n",
    "        # pad/trim\n",
    "        if len(seg) < total_ms:\n",
    "            seg += AudioSegment.silent(duration=(total_ms - len(seg)))\n",
    "        seg = seg[:total_ms]\n",
    "\n",
    "        # measure decalage\n",
    "        non2            = detect_nonsilent(seg, min_silence_len=1, silence_thresh=seg.dBFS-16)\n",
    "        start_audio_ms  = non2[0][0] if non2 else pre_ms\n",
    "        end_audio_ms    = non2[-1][1] if non2 else total_ms - post_ms\n",
    "        abs_start_audio = int(start_s*1000) + start_audio_ms\n",
    "        abs_end_audio   = int(start_s*1000) + end_audio_ms\n",
    "        abs_start_video = int(start_s*1000)\n",
    "        abs_end_video   = int(end_s*1000)\n",
    "        decal_start = abs_start_audio - abs_start_video\n",
    "        decal_end   = abs_end_audio   - abs_end_video\n",
    "\n",
    "        # optional warp\n",
    "        gen_dur = seg.duration_seconds\n",
    "        diff    = seg_dur - gen_dur\n",
    "        if abs(diff) > 0.20:\n",
    "            seg = change_playback_speed(seg, seg_dur/gen_dur)\n",
    "\n",
    "        # place on combined\n",
    "        start_ms = int(start_s*1000)\n",
    "        if len(combined) < start_ms:\n",
    "            combined += AudioSegment.silent(duration=(start_ms-len(combined)))\n",
    "        combined += seg\n",
    "\n",
    "        debug.append(\n",
    "            f\"Segment {idx+1} ({start_s:.2f}-{end_s:.2f}s): \"\n",
    "            f\"pre={pre_ms}ms, post={post_ms}ms, speed={rate}, \"\n",
    "            f\"décalage_start={decal_start}ms, décalage_end={decal_end}ms, \"\n",
    "            f\"inter={inter}, phrases={phrases}\\n\"\n",
    "        )\n",
    "\n",
    "    # write debug & export\n",
    "    with open(debug_log_file, \"w\", encoding=\"utf-8\") as df:\n",
    "        df.write(\"Translation Debug Log\\n\\n\")\n",
    "        df.writelines(debug)\n",
    "    combined.export(output_audio_path, format=\"wav\")\n",
    "    return output_audio_path\n",
    "\n",
    "# ── Merge & Main ────────────────────────────────────────────────────────────────\n",
    "def merge_audio_video():\n",
    "    video = VideoFileClip(input_video)\n",
    "    audio = AudioFileClip(translated_audio)\n",
    "    if audio.duration < video.duration:\n",
    "        extra = AudioSegment.silent(duration=(video.duration - audio.duration)*1000)\n",
    "        tmpf  = os.path.join(output_dir, \"temp_full.wav\")\n",
    "        AudioSegment.from_file(translated_audio).append(extra).export(tmpf, format=\"wav\")\n",
    "        audio = AudioFileClip(tmpf)\n",
    "    video.set_audio(audio).write_videofile(output_video,\n",
    "                                            codec=\"libx264\", audio_codec=\"aac\",\n",
    "                                            temp_audiofile=\"temp-audio.m4a\", remove_temp=True)\n",
    "\n",
    "async def async_main():\n",
    "    extract_audio()\n",
    "    _, segs = transcribe(extracted_audio)\n",
    "    generate_subtitle_file(segs, subtitle_file)\n",
    "    await async_generate_translated_audio_with_sync_using_review(\n",
    "        subtitle_file, translated_audio, debug_log_file, review_file\n",
    "    )\n",
    "    merge_audio_video()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Nous pouvons maintenant utiliser run_until_complete même si un loop tourne déjà\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(async_main())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
